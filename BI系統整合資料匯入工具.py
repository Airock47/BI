#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BIç³»çµ±æ•´åˆè³‡æ–™åŒ¯å…¥å·¥å…·
æ”¯æ´æ‰€æœ‰åˆ†æ”¯ç³»çµ±çš„è³‡æ–™åº«åŒ¯å…¥ï¼šéŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±ã€åº«å­˜æŸ¥è©¢ç³»çµ±ã€æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ±ã€CRMè¨˜äº‹
"""

import pandas as pd
import sqlite3
import os
from datetime import datetime
import time
import re

class BIIntegratedDataConverter:
    def __init__(self):
        # BIæ ¹ç›®éŒ„
        self.bi_root = os.path.dirname(__file__)

        # çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„
        self.data_source_dir = os.path.join(self.bi_root, "è³‡æ–™ä¾†æº")

        # æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ± (ProcureTrack)
        self.procuretrack_dir = os.path.join(self.bi_root, "ProcureTrack")
        self.procuretrack_db_dir = os.path.join(self.procuretrack_dir, "database")
        self.procuretrack_db_path = os.path.join(self.procuretrack_db_dir, "procure.db")

        # éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è·¯å¾‘
        self.sales_base_dir = os.path.join(
            self.bi_root, "Sales_information_inquiry", "database"
        )
        self.sales_db_path = os.path.join(self.sales_base_dir, "sales.db")
        self.repair_db_path = os.path.join(self.sales_base_dir, "repair.db")
        self.custody_db_path = os.path.join(self.sales_base_dir, "custody.db")
        self.customer_db_path = os.path.join(self.sales_base_dir, "customer_new.db")
        self.service_card_db_path = os.path.join(self.sales_base_dir, "service_card.db")

        # åº«å­˜æŸ¥è©¢ç³»çµ±è·¯å¾‘
        self.inventory_inquiry_base_dir = os.path.join(
            self.bi_root, "Inventory_inquiry_system", "database"
        )
        self.inventory_data_db_path = os.path.join(
            self.inventory_inquiry_base_dir, "inventory_data.db"
        )

        # CRM è³‡æ–™åº«ä½ç½®
        self.crm_dir = os.path.join(self.bi_root, "CRM")
        self.crm_db_path = os.path.join(self.crm_dir, "CRMè¨˜äº‹.db")
        self.crm_source_path = os.path.join(self.data_source_dir, "CRMè¨˜äº‹.xlsx")

        # AI åŠ©ç†è³‡æ–™åº«ä½ç½®
        self.ai_assistant_db_dir = os.path.join(self.bi_root, "ai_assistant", "database")
        self.sales_ai_db_path = os.path.join(self.ai_assistant_db_dir, "sales_ai.db")

        # ç¢ºä¿æ‰€æœ‰ç›®éŒ„å­˜åœ¨
        for db_dir in [
            self.data_source_dir,
            self.procuretrack_db_dir,
            self.sales_base_dir,
            self.inventory_inquiry_base_dir,
            self.crm_dir,
            self.ai_assistant_db_dir,
        ]:
            if not os.path.exists(db_dir):
                os.makedirs(db_dir)

    def log_time(self, message):
        """è¨˜éŒ„æ™‚é–“å’Œè¨Šæ¯"""
        current_time = datetime.now().strftime("%H:%M:%S")
        print(f"[{current_time}] {message}")

    def clean_date_string(self, date_str):
        """æ¸…ç†æ—¥æœŸå­—ä¸²ï¼Œç§»é™¤ä¸­æ–‡æ˜ŸæœŸå¹¾ç­‰å¹²æ“¾å­—ç¬¦"""
        if pd.isna(date_str) or date_str == "":
            return date_str

        date_str = str(date_str)

        # ç§»é™¤ä¸­æ–‡æ˜ŸæœŸå¹¾ï¼š(ä¸€), (äºŒ), (ä¸‰), (å››), (äº”), (å…­), (æ—¥)
        import re

        date_str = re.sub(r"\s*\([ä¸€äºŒä¸‰å››äº”å…­æ—¥]\)\s*", "", date_str)

        # ç§»é™¤å…¶ä»–å¯èƒ½çš„å¹²æ“¾å­—ç¬¦
        date_str = date_str.strip()

        return date_str

    def normalize_phone_value(self, value):
        """å°‡é›»è©±å­—ä¸²è½‰ç‚ºåƒ…åŒ…å«æ•¸å­—"""
        if pd.isna(value):
            return ""

        value_str = str(value).strip()
        if value_str.lower() in {"nan", "none"}:
            return ""

        digits_only = re.sub(r"\D", "", value_str)
        return digits_only

    def normalize_phone_columns(self, df):
        """å°‡è³‡æ–™è¡¨ä¸­çš„é›»è©±æ¬„ä½çµ±ä¸€ç§»é™¤éæ•¸å­—å­—å…ƒ"""
        if df is None or df.empty:
            return df

        phone_keywords = ["é›»è©±", "æ‰‹æ©Ÿ", "Phone", "phone", "TEL", "tel"]
        for col in df.columns:
            col_name = str(col)
            if any(keyword in col_name for keyword in phone_keywords):
                df[col] = df[col].apply(self.normalize_phone_value)

        return df

    def remove_hyphen(self, value):
        """ç§»é™¤å­—ä¸²ä¸­çš„é€£å­—è™Ÿ"""
        if pd.isna(value):
            return ""

        value_str = str(value).strip()
        if value_str.lower() in {"nan", "none"}:
            return ""

        return value_str.replace("-", "")

    def normalize_remark_columns(self, df, columns):
        """ç§»é™¤å‚™è¨»æ¬„ä½ä¸­é›»è©±çš„é€£å­—è™Ÿ"""
        if df is None or df.empty or not columns:
            return df

        def _clean_value(value):
            if pd.isna(value):
                return ""

            value_str = str(value).strip()
            if value_str.lower() in {"nan", "none"}:
                return ""

            return re.sub(r"(?<=\d)-(?=\d)", "", value_str)

        for col in columns:
            if col in df.columns:
                df[col] = df[col].apply(_clean_value)

        return df

    def convert_datetime_optimized(self, df, date_columns):
        """å„ªåŒ–çš„æ—¥æœŸæ™‚é–“è½‰æ›ï¼Œæ”¯æ´å¤šç¨®æ ¼å¼åŒ…æ‹¬ Excel åºåˆ—è™Ÿ"""
        for col in date_columns:
            if col in df.columns:
                self.log_time(f"ğŸ—“ï¸ è™•ç†æ—¥æœŸæ¬„ä½ï¼š{col}")

                # å…ˆæª¢æŸ¥åŸå§‹è³‡æ–™
                original_data = df[col].copy()
                non_null_count = original_data.notna().sum()
                self.log_time(
                    f"   åŸå§‹è³‡æ–™ï¼š{non_null_count}/{len(original_data)} ç­†æœ‰å€¼"
                )

                if non_null_count == 0:
                    self.log_time(f"   âš ï¸ {col} æ¬„ä½å…¨éƒ¨ç‚ºç©ºï¼Œè·³éè½‰æ›")
                    continue

                try:
                    converted = False

                    # æª¢æŸ¥è³‡æ–™é¡å‹
                    if pd.api.types.is_datetime64_any_dtype(original_data):
                        # å¦‚æœå·²ç¶“æ˜¯ datetime æ ¼å¼ï¼Œç›´æ¥è½‰æ›ç‚ºå­—ä¸²
                        df[col] = original_data.dt.strftime("%Y-%m-%d %H:%M:%S")
                        converted_count = df[col].notna().sum()
                        self.log_time(
                            f"   âœ… å·²æ˜¯datetimeæ ¼å¼ï¼Œç›´æ¥è½‰æ›ï¼š{converted_count} ç­†"
                        )
                        converted = True

                    # æ–¹æ³•1ï¼šæª¢æŸ¥æ˜¯å¦ç‚º Excel åºåˆ—è™Ÿï¼ˆæ•¸å­—æ ¼å¼çš„æ—¥æœŸï¼‰
                    elif original_data.dtype in [
                        "int64",
                        "float64",
                    ] or pd.api.types.is_numeric_dtype(original_data):
                        try:
                            # éæ¿¾æ‰æ˜é¡¯ä¸æ˜¯æ—¥æœŸçš„æ•¸å­—ï¼ˆå¤ªå°æˆ–å¤ªå¤§ï¼‰
                            numeric_data = pd.to_numeric(original_data, errors="coerce")
                            valid_range = (numeric_data >= 1) & (
                                numeric_data <= 100000
                            )  # Excel æ—¥æœŸç¯„åœ

                            if valid_range.any():
                                df[col] = pd.to_datetime(
                                    numeric_data,
                                    origin="1899-12-30",
                                    unit="D",
                                    errors="coerce",
                                )
                                converted_count = df[col].notna().sum()
                                if converted_count > 0:
                                    self.log_time(
                                        f"   âœ… Excelåºåˆ—è™Ÿè½‰æ›æˆåŠŸï¼š{converted_count} ç­†"
                                    )
                                    df[col] = df[col].dt.strftime("%Y-%m-%d %H:%M:%S")
                                    converted = True
                        except Exception as e:
                            self.log_time(f"   âš ï¸ Excelåºåˆ—è™Ÿè½‰æ›å¤±æ•—ï¼š{e}")

                    # æ–¹æ³•2ï¼šè™•ç†å­—ä¸²æ ¼å¼çš„æ—¥æœŸ
                    if not converted:
                        # å…ˆæ¸…ç†æ—¥æœŸå­—ä¸²
                        cleaned_data = original_data.apply(self.clean_date_string)
                        self.log_time(f"   ğŸ§¹ æ¸…ç†æ—¥æœŸå­—ä¸²å®Œæˆ")

                        # å˜—è©¦å¸¸è¦‹çš„æ—¥æœŸæ ¼å¼ï¼ˆå„ªå…ˆè™•ç†æ˜‡å³°éŠ·å”®è³‡æ–™çš„æ ¼å¼ï¼‰
                        formats_to_try = [
                            "%Y/%m/%d",  # æ˜‡å³°éŠ·å”®è³‡æ–™æ ¼å¼ï¼š2020/01/16
                            "%Y-%m-%d",  # æ¨™æº–æ ¼å¼
                            "%Y/%m/%d %H:%M:%S",  # æ˜‡å³°éŠ·å”®è³‡æ–™å¸¶æ™‚é–“
                            "%Y-%m-%d %H:%M:%S",  # æ¨™æº–æ ¼å¼å¸¶æ™‚é–“
                            "%m/%d/%Y",  # ç¾å¼æ ¼å¼
                            "%d/%m/%Y",  # æ­å¼æ ¼å¼
                            "%Y%m%d",  # ç·Šæ¹Šæ ¼å¼
                        ]

                        for fmt in formats_to_try:
                            try:
                                test_conversion = pd.to_datetime(
                                    cleaned_data, format=fmt, errors="coerce"
                                )
                                converted_count = test_conversion.notna().sum()
                                if converted_count > 0:
                                    df[col] = test_conversion.dt.strftime(
                                        "%Y-%m-%d %H:%M:%S"
                                    )
                                    self.log_time(
                                        f"   âœ… æ ¼å¼ {fmt} è½‰æ›æˆåŠŸï¼š{converted_count} ç­†"
                                    )
                                    converted = True
                                    break
                                else:
                                    # é¡¯ç¤ºç‚ºä»€éº¼é€™å€‹æ ¼å¼æ²’æœ‰è½‰æ›æˆåŠŸ
                                    sample_data = cleaned_data.dropna().head(3)
                                    if len(sample_data) > 0:
                                        self.log_time(
                                            f"   âš ï¸ æ ¼å¼ {fmt} ç„¡æ³•è½‰æ›ï¼Œæ¨£æœ¬æ•¸æ“šï¼š{list(sample_data)}"
                                        )
                            except Exception as e:
                                self.log_time(f"   âš ï¸ æ ¼å¼ {fmt} è½‰æ›ç•°å¸¸ï¼š{e}")
                                continue

                    # æ–¹æ³•3ï¼šä½¿ç”¨ pandas é è¨­è½‰æ›
                    if not converted:
                        try:
                            cleaned_data = original_data.apply(self.clean_date_string)
                            test_conversion = pd.to_datetime(
                                cleaned_data, errors="coerce"
                            )
                            converted_count = test_conversion.notna().sum()
                            if converted_count > 0:
                                df[col] = test_conversion.dt.strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                )
                                self.log_time(
                                    f"   âœ… é è¨­è½‰æ›æˆåŠŸï¼š{converted_count} ç­†"
                                )
                                converted = True
                        except Exception as e:
                            self.log_time(f"   âš ï¸ é è¨­è½‰æ›å¤±æ•—ï¼š{e}")

                    # æœ€çµ‚æª¢æŸ¥
                    if converted:
                        final_count = df[col].notna().sum()
                        self.log_time(f"   ğŸ“Š æœ€çµ‚è½‰æ›çµæœï¼š{final_count} ç­†æœ‰æ•ˆæ—¥æœŸ")
                    else:
                        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—ï¼Œä¿æŒåŸæ¨£ä½†è½‰ç‚ºå­—ä¸²
                        df[col] = original_data.astype(str)
                        self.log_time(f"   âš ï¸ æ—¥æœŸè½‰æ›å¤±æ•—ï¼Œä¿æŒåŸå§‹æ ¼å¼")

                except Exception as e:
                    self.log_time(f"   âŒ {col} è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    df[col] = original_data.astype(str)

        return df

    def convert_shengfeng_dates(self, df, date_columns):
        """å°ˆé–€è™•ç†æ˜‡å³°éŠ·å”®è³‡æ–™çš„æ—¥æœŸè½‰æ›"""
        for col in date_columns:
            if col in df.columns:
                self.log_time(f"ğŸ—“ï¸ è™•ç†æ˜‡å³°éŠ·å”®è³‡æ–™æ—¥æœŸæ¬„ä½ï¼š{col}")

                original_data = df[col].copy()
                non_null_count = original_data.notna().sum()
                self.log_time(
                    f"   åŸå§‹è³‡æ–™ï¼š{non_null_count}/{len(original_data)} ç­†æœ‰å€¼"
                )

                if non_null_count == 0:
                    self.log_time(f"   âš ï¸ {col} æ¬„ä½å…¨éƒ¨ç‚ºç©ºï¼Œè·³éè½‰æ›")
                    continue

                try:
                    # é¡¯ç¤ºåŸå§‹æ•¸æ“šæ¨£æœ¬
                    sample_data = original_data.dropna().head(5)
                    self.log_time(f"   ğŸ“… åŸå§‹æ—¥æœŸæ¨£æœ¬ï¼š{list(sample_data)}")

                    converted = False

                    # æ–¹æ³•1ï¼šç›´æ¥è™•ç†YYYY/MM/DDæ ¼å¼
                    if not converted:
                        try:
                            # å…ˆè½‰æ›ç‚ºå­—ç¬¦ä¸²ä¸¦æ¸…ç†
                            str_data = original_data.astype(str).str.strip()

                            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…YYYY/MM/DDæ ¼å¼
                            import re

                            date_pattern = r"^\d{4}/\d{1,2}/\d{1,2}$"
                            valid_dates = str_data[
                                str_data.str.match(date_pattern, na=False)
                            ]

                            if len(valid_dates) > 0:
                                # è½‰æ›YYYY/MM/DDæ ¼å¼
                                converted_dates = pd.to_datetime(
                                    valid_dates, format="%Y/%m/%d", errors="coerce"
                                )

                                # æ›´æ–°åŸå§‹æ•¸æ“š
                                df.loc[converted_dates.index, col] = (
                                    converted_dates.dt.strftime("%Y-%m-%d %H:%M:%S")
                                )

                                converted_count = converted_dates.notna().sum()
                                self.log_time(
                                    f"   âœ… YYYY/MM/DDæ ¼å¼è½‰æ›æˆåŠŸï¼š{converted_count} ç­†"
                                )
                                converted = True
                        except Exception as e:
                            self.log_time(f"   âš ï¸ YYYY/MM/DDæ ¼å¼è½‰æ›å¤±æ•—ï¼š{e}")

                    # æ–¹æ³•2ï¼šä½¿ç”¨pandasçš„æ™ºèƒ½è½‰æ›
                    if not converted:
                        try:
                            test_conversion = pd.to_datetime(
                                original_data, errors="coerce"
                            )
                            converted_count = test_conversion.notna().sum()
                            if converted_count > 0:
                                df[col] = test_conversion.dt.strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                )
                                self.log_time(
                                    f"   âœ… æ™ºèƒ½è½‰æ›æˆåŠŸï¼š{converted_count} ç­†"
                                )
                                converted = True
                        except Exception as e:
                            self.log_time(f"   âš ï¸ æ™ºèƒ½è½‰æ›å¤±æ•—ï¼š{e}")

                    # æœ€çµ‚æª¢æŸ¥
                    if converted:
                        final_count = df[col].notna().sum()
                        self.log_time(f"   ğŸ“Š æœ€çµ‚è½‰æ›çµæœï¼š{final_count} ç­†æœ‰æ•ˆæ—¥æœŸ")

                        # é¡¯ç¤ºè½‰æ›å¾Œçš„æ¨£æœ¬
                        converted_sample = df[col].dropna().head(3)
                        self.log_time(f"   ğŸ“… è½‰æ›å¾Œæ¨£æœ¬ï¼š{list(converted_sample)}")
                    else:
                        self.log_time(f"   âš ï¸ æ—¥æœŸè½‰æ›å¤±æ•—ï¼Œä¿æŒåŸå§‹æ ¼å¼")
                        df[col] = original_data.astype(str)

                except Exception as e:
                    self.log_time(f"   âŒ {col} è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    df[col] = original_data.astype(str)

        return df

    def create_indexes_batch(self, cursor, table_name, index_configs):
        """æ‰¹é‡å»ºç«‹ç´¢å¼•"""
        for column, index_name in index_configs:
            try:
                cursor.execute(
                    f"CREATE INDEX IF NOT EXISTS {index_name} ON {table_name}({column})"
                )
            except Exception as e:
                print(f"å»ºç«‹ç´¢å¼• {index_name} å¤±æ•—: {e}")

    def insert_data_in_chunks(self, df, conn, table_name, chunk_size=1000):
        """åˆ†æ‰¹æ’å…¥è³‡æ–™ï¼Œé¿å… SQLite è®Šæ•¸é™åˆ¶"""
        total_rows = len(df)
        self.log_time(
            f"ğŸ“Š é–‹å§‹åˆ†æ‰¹æ’å…¥ {table_name}ï¼šç¸½å…± {total_rows:,} ç­†ï¼Œæ¯æ‰¹ {chunk_size:,} ç­†"
        )

        # å…ˆåˆªé™¤èˆŠè¡¨
        cursor = conn.cursor()
        cursor.execute(f"DROP TABLE IF EXISTS {table_name}")

        # åˆ†æ‰¹æ’å…¥
        for i in range(0, total_rows, chunk_size):
            chunk = df.iloc[i : i + chunk_size]
            if i == 0:
                # ç¬¬ä¸€æ‰¹å»ºç«‹è¡¨æ ¼
                chunk.to_sql(table_name, conn, if_exists="replace", index=False)
            else:
                # å¾ŒçºŒæ‰¹æ¬¡è¿½åŠ è³‡æ–™
                chunk.to_sql(table_name, conn, if_exists="append", index=False)

            progress = min(i + chunk_size, total_rows)
            self.log_time(
                f"ğŸ“ˆ {table_name} é€²åº¦ï¼š{progress:,}/{total_rows:,} ({progress/total_rows*100:.1f}%)"
            )

        self.log_time(f"âœ… {table_name} æ’å…¥å®Œæˆ")

    def convert_customer_data(self):
        """è½‰æ›å®¢æˆ¶è³‡æ–™ - ä½¿ç”¨å®¢æˆ¶è³‡æ–™.xlsx"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›å®¢æˆ¶è³‡æ–™...")

        try:
            # åˆªé™¤èˆŠçš„å®¢æˆ¶è³‡æ–™åº«
            if os.path.exists(self.customer_db_path):
                os.remove(self.customer_db_path)

            # è®€å–å®¢æˆ¶è³‡æ–™.xlsxï¼ˆå¾çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„ï¼‰
            customer_xlsx_path = os.path.join(self.data_source_dir, "å®¢æˆ¶è³‡æ–™.xlsx")
            if not os.path.exists(customer_xlsx_path):
                self.log_time(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{customer_xlsx_path}")
                return

            # è®€å–Excelæª”æ¡ˆ
            df_customer = pd.read_excel(customer_xlsx_path)
            self.log_time(f"ğŸ“Š è®€å–å®¢æˆ¶è³‡æ–™ï¼š{len(df_customer)} ç­†")

            # å»ºç«‹æ–°çš„å®¢æˆ¶è³‡æ–™åº«
            conn = sqlite3.connect(self.customer_db_path)
            cursor = conn.cursor()

            # å»ºç«‹å®¢æˆ¶åŸºæœ¬è³‡æ–™è¡¨
            cursor.execute(
                """
                CREATE TABLE customer_basic (
                    å®¢æˆ¶ä»£ç¢¼ TEXT PRIMARY KEY,
                    å®¢æˆ¶åç¨± TEXT,
                    è¯çµ¡åœ°å€ TEXT,
                    éƒµéå€è™Ÿ TEXT,
                    è¯çµ¡é›»è©± TEXT,
                    è¯çµ¡äºº TEXT,
                    æ¥­å‹™äººå“¡åç¨± TEXT,
                    éŠ·å”®åˆ†é¡ç¢¼åç¨± TEXT,
                    é è¨­éŠ·å”®é€šè·¯åç¨± TEXT,
                    æ­£èˆªèˆŠç·¨ç¢¼ TEXT,
                    èˆŠç·¨ç¢¼ TEXT,
                    å»ºç«‹æ™‚é–“ DATETIME DEFAULT CURRENT_TIMESTAMP,
                    æ›´æ–°æ™‚é–“ DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # å»ºç«‹å®¢æˆ¶è¯çµ¡äººè¡¨
            cursor.execute(
                """
                CREATE TABLE customer_contacts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    å®¢æˆ¶ä»£ç¢¼ TEXT,
                    è¯çµ¡äººå§“å TEXT,
                    è¯çµ¡é›»è©± TEXT,
                    è¯çµ¡æ‰‹æ©Ÿ TEXT,
                    é›»å­éƒµä»¶ TEXT,
                    è·ç¨± TEXT,
                    å‚™è¨» TEXT,
                    FOREIGN KEY (å®¢æˆ¶ä»£ç¢¼) REFERENCES customer_basic(å®¢æˆ¶ä»£ç¢¼)
                )
            """
            )

            # å»ºç«‹å®¢æˆ¶é€è²¨åœ°å€è¡¨
            cursor.execute(
                """
                CREATE TABLE customer_addresses (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    å®¢æˆ¶ä»£ç¢¼ TEXT,
                    åœ°å€åç¨± TEXT,
                    é€è²¨åœ°å€ TEXT,
                    éƒµéå€è™Ÿ TEXT,
                    è¯çµ¡äºº TEXT,
                    è¯çµ¡é›»è©± TEXT,
                    æ˜¯å¦é è¨­ INTEGER DEFAULT 0,
                    å‚™è¨» TEXT,
                    FOREIGN KEY (å®¢æˆ¶ä»£ç¢¼) REFERENCES customer_basic(å®¢æˆ¶ä»£ç¢¼)
                )
            """
            )

            # è™•ç†å®¢æˆ¶è³‡æ–™æ¬„ä½å°æ‡‰
            column_mapping = {
                "å®¢æˆ¶ä»£ç¢¼": ["å®¢æˆ¶ä»£ç¢¼", "Unnamed: 0"],
                "å®¢æˆ¶åç¨±": ["å®¢æˆ¶åç¨±", "Unnamed: 3"],
                "è¯çµ¡åœ°å€": ["è¯çµ¡åœ°å€", "Unnamed: 2"],
                "éƒµéå€è™Ÿ": ["éƒµéå€è™Ÿ", "Unnamed: 1"],
                "è¯çµ¡é›»è©±": ["è¯çµ¡é›»è©±", "Unnamed: 10"],
                "è¯çµ¡äºº": ["è¯çµ¡äºº", "Unnamed: 9"],
                "æ¥­å‹™äººå“¡åç¨±": ["æ¥­å‹™äººå“¡åç¨±", "Unnamed: 4"],
                "éŠ·å”®åˆ†é¡ç¢¼åç¨±": ["éŠ·å”®åˆ†é¡ç¢¼åç¨±", "Unnamed: 5"],
                "é è¨­éŠ·å”®é€šè·¯åç¨±": ["é è¨­éŠ·å”®é€šè·¯åç¨±", "Unnamed: 6"],
                "æ­£èˆªèˆŠç·¨ç¢¼": ["æ­£èˆªèˆŠç·¨ç¢¼", "Unnamed: 7"],
                "èˆŠç·¨ç¢¼": ["èˆŠç·¨ç¢¼", "Unnamed: 8"],
            }

            # å»ºç«‹æ¨™æº–åŒ–çš„å®¢æˆ¶è³‡æ–™ DataFrame
            customer_df = pd.DataFrame()
            for target_col, possible_cols in column_mapping.items():
                for col in possible_cols:
                    if col in df_customer.columns:
                        customer_df[target_col] = (
                            df_customer[col]
                            .astype(str)
                            .replace("nan", "")
                            .replace("None", "")
                        )
                        break
                if target_col not in customer_df.columns:
                    customer_df[target_col] = ""

            # éæ¿¾æ‰ç„¡æ•ˆçš„è³‡æ–™è¡Œ
            customer_df = customer_df[
                (customer_df["å®¢æˆ¶ä»£ç¢¼"] != "")
                & (customer_df["å®¢æˆ¶ä»£ç¢¼"] != "å®¢æˆ¶è³‡æ–™")
                & (customer_df["å®¢æˆ¶ä»£ç¢¼"] != "ä»£ç¢¼")
                & (customer_df["å®¢æˆ¶ä»£ç¢¼"].notna())
            ]

            customer_df = self.normalize_phone_columns(customer_df)

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            self.insert_data_in_chunks(
                customer_df, conn, "customer_basic", chunk_size=1000
            )

            # å»ºç«‹ç´¢å¼•
            index_configs = [
                ("å®¢æˆ¶åç¨±", "idx_customer_name"),
                ("å®¢æˆ¶ä»£ç¢¼", "idx_customer_code"),
                ("è¯çµ¡é›»è©±", "idx_customer_phone"),
                ("è¯çµ¡åœ°å€", "idx_customer_address"),
            ]
            self.create_indexes_batch(cursor, "customer_basic", index_configs)

            conn.commit()
            conn.close()

            self.log_time(f"âœ… å®¢æˆ¶è³‡æ–™è½‰æ›å®Œæˆï¼š{len(customer_df)} ç­†")

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›å®¢æˆ¶è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_sales_data(self):
        """è½‰æ›éŠ·å”®è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›éŠ·å”®è³‡æ–™...")

        try:
            # è®€å–éŠ·å”®è³‡æ–™æª”æ¡ˆï¼ˆå¾çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„ï¼‰
            df_shipment_path = os.path.join(self.data_source_dir, "ç™¼è²¨ç‹€æ³åˆ†æè¡¨.xlsx")
            df_shengfeng_path = os.path.join(self.data_source_dir, "æ˜‡å³°éŠ·å”®è³‡æ–™.xlsx")

            df_list = []

            if os.path.exists(df_shipment_path):
                df_shipment = pd.read_excel(df_shipment_path)
                df_list.append(df_shipment)
                self.log_time(f"ğŸ“Š ç™¼è²¨ç‹€æ³åˆ†æè¡¨ï¼š{len(df_shipment)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shipment_path}")

            if os.path.exists(df_shengfeng_path):
                df_shengfeng = pd.read_excel(df_shengfeng_path)
                df_list.append(df_shengfeng)
                self.log_time(f"ğŸ“Š æ˜‡å³°éŠ·å”®è³‡æ–™ï¼š{len(df_shengfeng)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shengfeng_path}")

            if not df_list:
                self.log_time("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•éŠ·å”®è³‡æ–™æª”æ¡ˆ")
                return

            # è½‰æ›æ—¥æœŸæ™‚é–“
            date_columns = ["ç™¼è²¨æ—¥æœŸ", "å–®æ“šæ—¥æœŸ"]
            for i, df in enumerate(df_list):
                if i == 0:  # ç™¼è²¨ç‹€æ³åˆ†æè¡¨
                    df_list[i] = self.convert_datetime_optimized(df, date_columns)
                else:  # æ˜‡å³°éŠ·å”®è³‡æ–™
                    df_list[i] = self.convert_shengfeng_dates(df, date_columns)

            # åˆä½µè³‡æ–™
            df_combined = pd.concat(df_list, ignore_index=True)
            df_combined = self.normalize_remark_columns(df_combined, ["å‚™è¨»"])
            self.log_time(f"ğŸ“Š åˆä½µå¾Œç¸½ç­†æ•¸ï¼š{len(df_combined)}")

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.sales_db_path)

            # å„²å­˜ä¸»æª”è³‡æ–™ï¼ˆå»é‡è¤‡ï¼‰
            main_columns = [
                "å–®æ“šç·¨è™Ÿ",
                "ç™¼è²¨æ—¥æœŸ",
                "å®¢æˆ¶åç¨±",
                "é€è²¨åœ°å€",
                "è¯çµ¡é›»è©±",
                "å‚™è¨»",
                "æ¥­å‹™äººå“¡åç¨±",
                "å®¢æˆ¶ä»£ç¢¼",
            ]
            # çµ±ä¸€è£œé½Šèˆ‡æ’åºï¼Œç¢ºä¿å„ªå…ˆä¿ç•™å«ã€Œå®¢æˆ¶åç¨±ã€ä¹‹ç´€éŒ„
            try:
                if "å®¢æˆ¶åç¨±" in df_combined.columns:
                    df_combined["å®¢æˆ¶åç¨±"] = (
                        df_combined["å®¢æˆ¶åç¨±"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å®¢æˆ¶" in df_combined.columns:
                    df_combined["å®¢æˆ¶"] = (
                        df_combined["å®¢æˆ¶"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å®¢æˆ¶åç¨±" not in df_combined.columns and "å®¢æˆ¶" in df_combined.columns:
                    df_combined["å®¢æˆ¶åç¨±"] = df_combined["å®¢æˆ¶"]
                elif "å®¢æˆ¶åç¨±" in df_combined.columns and "å®¢æˆ¶" in df_combined.columns:
                    mask_empty = df_combined["å®¢æˆ¶åç¨±"].isin(["", "nan", "None"]) | df_combined["å®¢æˆ¶åç¨±"].isna()
                    df_combined.loc[mask_empty, "å®¢æˆ¶åç¨±"] = df_combined.loc[mask_empty, "å®¢æˆ¶"].fillna("")

                if "å–®æ“šç·¨è™Ÿ" in df_combined.columns and "å®¢æˆ¶åç¨±" in df_combined.columns:
                    df_combined["__has_name__"] = df_combined["å®¢æˆ¶åç¨±"].astype(str).str.strip().ne("")
                    df_combined.sort_values(["å–®æ“šç·¨è™Ÿ", "__has_name__"], ascending=[True, False], inplace=True)
            except Exception:
                pass

            available_main_columns = [
                col for col in main_columns if col in df_combined.columns
            ]
            df_main = df_combined[available_main_columns].drop_duplicates(
                subset=["å–®æ“šç·¨è™Ÿ"]
            )

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            self.insert_data_in_chunks(df_main, conn, "sales_main", chunk_size=1000)

            # å„²å­˜æ˜ç´°è³‡æ–™
            detail_columns = ["å–®æ“šç·¨è™Ÿ", "ç”¢å“åç¨±", "äº¤æ˜“æ•¸é‡", "äº¤æ˜“åƒ¹", "å«ç¨…é‡‘é¡"]
            available_detail_columns = [
                col for col in detail_columns if col in df_combined.columns
            ]
            df_detail = df_combined[available_detail_columns]

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            self.insert_data_in_chunks(df_detail, conn, "sales_detail", chunk_size=1000)

            # å»ºç«‹ç´¢å¼•
            cursor = conn.cursor()
            sales_indexes = [
                ("å–®æ“šç·¨è™Ÿ", "idx_sales_main_doc_no"),
                ("å®¢æˆ¶åç¨±", "idx_sales_main_customer"),
                ("é€è²¨åœ°å€", "idx_sales_main_address"),
                ("è¯çµ¡é›»è©±", "idx_sales_main_phone"),
                ("å‚™è¨»", "idx_sales_main_remark"),
                ("å®¢æˆ¶ä»£ç¢¼", "idx_sales_main_customer_code"),
            ]
            self.create_indexes_batch(cursor, "sales_main", sales_indexes)

            detail_indexes = [("å–®æ“šç·¨è™Ÿ", "idx_sales_detail_doc_no")]
            self.create_indexes_batch(cursor, "sales_detail", detail_indexes)

            conn.commit()
            conn.close()

            self.log_time(
                f"âœ… éŠ·å”®è³‡æ–™è½‰æ›å®Œæˆï¼šä¸»æª” {len(df_main)} ç­†ï¼Œæ˜ç´° {len(df_detail)} ç­†"
            )

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›éŠ·å”®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_repair_data(self):
        """è½‰æ›ç¶­ä¿®è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›ç¶­ä¿®è³‡æ–™...")

        try:
            # è®€å–ç¶­ä¿®è³‡æ–™æª”æ¡ˆï¼ˆå¾çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„ï¼‰
            df_repair_path = os.path.join(self.data_source_dir, "ç¶­ä¿®è³‡æ–™.xlsx")
            df_shengfeng_path = os.path.join(self.data_source_dir, "æ˜‡å³°ç¶­ä¿®è³‡æ–™.xlsx")

            df_list = []

            if os.path.exists(df_repair_path):
                df_repair = pd.read_excel(df_repair_path)
                df_list.append(df_repair)
                self.log_time(f"ğŸ“Š ç¶­ä¿®è³‡æ–™ï¼š{len(df_repair)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_repair_path}")

            if os.path.exists(df_shengfeng_path):
                df_shengfeng = pd.read_excel(df_shengfeng_path)
                df_list.append(df_shengfeng)
                self.log_time(f"ğŸ“Š æ˜‡å³°ç¶­ä¿®è³‡æ–™ï¼š{len(df_shengfeng)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shengfeng_path}")

            if not df_list:
                self.log_time("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç¶­ä¿®è³‡æ–™æª”æ¡ˆ")
                return

            # è™•ç†å®¢æˆ¶æ¬„ä½ï¼ˆé¿å…æµ®é»æ•¸é¡¯ç¤ºï¼‰
            for df in df_list:
                if "å®¢æˆ¶" in df.columns:
                    df["å®¢æˆ¶"] = (
                        df["å®¢æˆ¶"].astype(str).replace("nan", "").replace("None", "")
                    )
                    # å»é™¤æµ®é»æ•¸çš„å°æ•¸éƒ¨åˆ†
                    df["å®¢æˆ¶"] = df["å®¢æˆ¶"].apply(
                        lambda x: x.split(".")[0] if "." in str(x) else x
                    )

            # è½‰æ›æ—¥æœŸæ™‚é–“
            date_columns = ["å‡ºå‹¤é–‹å§‹æ™‚é–“", "ç™¼è²¨æ—¥æœŸ", "å–®æ“šæ—¥æœŸ"]
            for i, df in enumerate(df_list):
                df_list[i] = self.convert_datetime_optimized(df, date_columns)

            # åˆä½µè³‡æ–™
            df_combined = pd.concat(df_list, ignore_index=True)
            df_combined = self.normalize_remark_columns(df_combined, ["å‚™è¨»"])
            self.log_time(f"ğŸ“Š åˆä½µå¾Œç¸½ç­†æ•¸ï¼š{len(df_combined)}")

            # æ·»åŠ å®¢æˆ¶ä»£ç¢¼æ¬„ä½
            if "å®¢æˆ¶" in df_combined.columns and "å®¢æˆ¶ä»£ç¢¼" not in df_combined.columns:
                df_combined["å®¢æˆ¶ä»£ç¢¼"] = df_combined["å®¢æˆ¶"]

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            # normalize customer name for repair
            try:
                if "å®¢æˆ¶åç¨±" in df_combined.columns:
                    df_combined["å®¢æˆ¶åç¨±"] = (
                        df_combined["å®¢æˆ¶åç¨±"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å®¢æˆ¶" in df_combined.columns:
                    df_combined["å®¢æˆ¶"] = (
                        df_combined["å®¢æˆ¶"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å®¢æˆ¶åç¨±" not in df_combined.columns and "å®¢æˆ¶" in df_combined.columns:
                    df_combined["å®¢æˆ¶åç¨±"] = df_combined["å®¢æˆ¶"]
                elif "å®¢æˆ¶åç¨±" in df_combined.columns and "å®¢æˆ¶" in df_combined.columns:
                    __mask_empty = df_combined["å®¢æˆ¶åç¨±"].isin(["", "nan", "None"]) | df_combined["å®¢æˆ¶åç¨±"].isna()
                    df_combined.loc[__mask_empty, "å®¢æˆ¶åç¨±"] = df_combined.loc[__mask_empty, "å®¢æˆ¶"].fillna("")
                if "å–®æ“šç·¨è™Ÿ" in df_combined.columns and "å®¢æˆ¶åç¨±" in df_combined.columns:
                    df_combined["__has_name__"] = df_combined["å®¢æˆ¶åç¨±"].astype(str).str.strip().ne("")
                    df_combined.sort_values(["å–®æ“šç·¨è™Ÿ", "__has_name__"], ascending=[True, False], inplace=True)
                    df_combined = df_combined.drop_duplicates(subset=["å–®æ“šç·¨è™Ÿ"], keep="first")
                    if "__has_name__" in df_combined.columns:
                        df_combined.drop(columns=["__has_name__"], inplace=True)
            except Exception:
                pass

            conn = sqlite3.connect(self.repair_db_path)

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            self.insert_data_in_chunks(
                df_combined, conn, "repair_data", chunk_size=1000
            )

            # å»ºç«‹ç´¢å¼•
            cursor = conn.cursor()
            repair_indexes = [
                ("å–®æ“šç·¨è™Ÿ", "idx_repair_doc_no"),
                ("å®¢æˆ¶åç¨±", "idx_repair_customer"),
                ("æœå‹™åœ°å€", "idx_repair_address"),
                ("è¯çµ¡é›»è©±", "idx_repair_phone"),
                ("å‚™è¨»", "idx_repair_remark"),
                ("å®¢æˆ¶ä»£ç¢¼", "idx_repair_customer_code"),
            ]
            self.create_indexes_batch(cursor, "repair_data", repair_indexes)

            conn.commit()
            conn.close()

            self.log_time(f"âœ… ç¶­ä¿®è³‡æ–™è½‰æ›å®Œæˆï¼š{len(df_combined)} ç­†")

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›ç¶­ä¿®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_crm_notes(self):
        """è½‰æ› CRMè¨˜äº‹.xlsx ç‚º SQLite"""
        self.log_time("?? é–‹å§‹è½‰æ› CRMè¨˜äº‹.xlsx ...")

        try:
            if not os.path.exists(self.crm_source_path):
                self.log_time(f"?? æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{self.crm_source_path}")
                return False

            os.makedirs(self.crm_dir, exist_ok=True)

            df = pd.read_excel(self.crm_source_path)
            self.log_time(f"?? CRMè¨˜äº‹åŸå§‹åˆ—æ•¸ï¼š{len(df)} ç­†")

            normalized_columns = []
            for idx, col in enumerate(df.columns):
                if pd.isna(col) or str(col).strip() == "":
                    normalized_columns.append(f"æ¬„ä½_{idx + 1}")
                else:
                    normalized_columns.append(str(col).strip())
            df.columns = normalized_columns

            before_drop = len(df)
            df = df.dropna(how="all")
            self.log_time(f"?? æ¸…ç†ç©ºç™½åˆ—å¾Œï¼š{before_drop} ç­† -> {len(df)} ç­†")

            conn = sqlite3.connect(self.crm_db_path)
            df.to_sql('crm_notes', conn, if_exists='replace', index=False)
            conn.close()

            self.log_time(f"? CRMè¨˜äº‹å¯«å…¥ SQLite å®Œæˆï¼Œå…± {len(df)} ç­†")
            return True

        except Exception as e:
            self.log_time(f"? è½‰æ› CRMè¨˜äº‹.xlsx ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return False
    def convert_inventory_inquiry_data(self):
        """è½‰æ›æ­£èˆªåº«å­˜è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›æ­£èˆªåº«å­˜è³‡æ–™...")

        try:
            # è®€å–Excelæª”æ¡ˆï¼ˆå¾çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„ï¼‰
            excel_path = os.path.join(self.data_source_dir, "æ­£èˆªåº«å­˜è³‡æ–™.xlsx")
            if not os.path.exists(excel_path):
                self.log_time(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{excel_path}")
                return False

            df = pd.read_excel(excel_path)
            self.log_time(f"ğŸ“Š è®€å–åˆ° {len(df)} ç­†åº«å­˜è³‡æ–™")

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.inventory_data_db_path)
            cursor = conn.cursor()

            # åˆªé™¤èˆŠè¡¨æ ¼ä¸¦é‡æ–°å»ºç«‹
            cursor.execute("DROP TABLE IF EXISTS inventory_data")

            # å»ºç«‹åº«å­˜è³‡æ–™è¡¨ï¼ˆå¯¦éš›æ’å…¥æ™‚æœƒç”± DataFrame é€é to_sql é‡å»ºï¼Œæ­¤çµæ§‹åƒ…ä¾›åƒè€ƒï¼‰
            cursor.execute(
                """
                CREATE TABLE inventory_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    product_name TEXT,
                    warehouse_name TEXT,
                    warehouse_partner_name TEXT,
                    inventory_type TEXT,
                    product_code TEXT,
                    specification TEXT,
                    unit TEXT,
                    quantity REAL,
                    unit_price REAL,
                    total_amount REAL,
                    last_update_date TEXT,
                    remark TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # è™•ç†è³‡æ–™æ¬„ä½å°æ‡‰ï¼ˆæ ¹æ“šå¯¦éš›Excelçµæ§‹èª¿æ•´ï¼‰
            # å…ˆæª¢æŸ¥Excelæª”æ¡ˆçš„æ¬„ä½çµæ§‹
            self.log_time(f"ğŸ“‹ Excelæ¬„ä½ï¼š{list(df.columns)}")

            # å¦‚æœExcelæª”æ¡ˆæœ‰è³‡æ–™ï¼Œé€²è¡Œè½‰æ›
            if len(df) > 0:
                # å»ºç«‹æ¬„ä½å°æ‡‰å­—å…¸
                column_mapping = {}
                for col in df.columns:
                    if "ç”¢å“åç¨±" in col:
                        column_mapping[col] = "product_name"
                    elif "å€‰åº«åç¨±" in col:
                        column_mapping[col] = "warehouse_name"
                    elif "å€‰åº«å¾€ä¾†å°è±¡åç¨±" in col or "å¾€ä¾†å°è±¡" in col:
                        column_mapping[col] = "warehouse_partner_name"
                    elif "å­˜è²¨å±¬æ€§" in col or "åº«å­˜å±¬æ€§" in col or col == "å±¬æ€§":
                        # ç›´æ¥æ²¿ç”¨ Excel çš„å­˜è²¨å±¬æ€§ï¼Œä¸åšé‚è¼¯æ›ç®—
                        column_mapping[col] = "inventory_type"
                    elif "ç”¢å“ä»£ç¢¼" in col or "ä»£ç¢¼" in col:
                        column_mapping[col] = "product_code"
                    elif "è¦æ ¼" in col:
                        column_mapping[col] = "specification"
                    elif "å–®ä½" in col:
                        column_mapping[col] = "unit"
                    elif "æ•¸é‡" in col:
                        column_mapping[col] = "quantity"
                    elif "å–®åƒ¹" in col:
                        column_mapping[col] = "unit_price"
                    elif "ç¸½é‡‘é¡" in col or "é‡‘é¡" in col:
                        column_mapping[col] = "total_amount"
                    elif "æ›´æ–°æ—¥æœŸ" in col or "æ—¥æœŸ" in col:
                        column_mapping[col] = "last_update_date"
                    elif "å‚™è¨»" in col:
                        column_mapping[col] = "remark"

                # é‡æ–°å‘½åæ¬„ä½
                df_mapped = df.rename(columns=column_mapping)

                # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å¡«å…¥ç©ºå€¼
                required_columns = [
                    "product_name",
                    "warehouse_name",
                    "warehouse_partner_name",
                    "inventory_type",
                    "product_code",
                    "specification",
                    "unit",
                    "quantity",
                    "unit_price",
                    "total_amount",
                    "last_update_date",
                    "remark",
                ]

                for col in required_columns:
                    if col not in df_mapped.columns:
                        df_mapped[col] = ""

                # åªä¿ç•™éœ€è¦çš„æ¬„ä½
                df_final = df_mapped[required_columns]

                df_final = self.normalize_remark_columns(df_final, ["remark"])
                # å­˜è²¨å±¬æ€§æ¬„ä½æ¨™æº–åŒ– + é¡¯ç¤ºç”¨æ–‡å­—å°æ‡‰
                # è‡ªæœ‰åº«å­˜ -> ä¸–ç£Šã€å€Ÿå…¥åº«å­˜ -> å¯„å€‰ã€å€Ÿå‡ºåº«å­˜ -> å€Ÿå‡º
                if "inventory_type" in df_final.columns:
                    inv = df_final["inventory_type"].fillna("").astype(str).str.strip()
                    mapping = {
                        "è‡ªæœ‰åº«å­˜": "ä¸–ç£Š",
                        "å€Ÿå…¥åº«å­˜": "å¯„å€‰",
                        "å€Ÿå‡ºåº«å­˜": "å€Ÿå‡º",
                        # ä¹Ÿæ”¯æ´å¯èƒ½çš„ç°¡å¯«
                        "è‡ªæœ‰": "ä¸–ç£Š",
                        "å€Ÿå…¥": "å¯„å€‰",
                        "å€Ÿå‡º": "å€Ÿå‡º",
                    }
                    df_final["inventory_type"] = inv.replace(mapping)

                # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
                self.insert_data_in_chunks(
                    df_final, conn, "inventory_data", chunk_size=1000
                )

                # å»ºç«‹ç´¢å¼•ï¼ˆé‡å°æŸ¥è©¢éœ€æ±‚ï¼‰
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_product_name ON inventory_data(product_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_warehouse_name ON inventory_data(warehouse_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_warehouse_partner_name ON inventory_data(warehouse_partner_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_inventory_type ON inventory_data(inventory_type)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_product_code ON inventory_data(product_code)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_quantity ON inventory_data(quantity)"
                )

                # å»ºç«‹è¤‡åˆç´¢å¼•ï¼ˆæå‡å¤šæ¬„ä½æŸ¥è©¢æ•ˆèƒ½ï¼‰
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_product_warehouse ON inventory_data(product_name, warehouse_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_warehouse_partner ON inventory_data(warehouse_name, warehouse_partner_name)"
                )

            # æäº¤ä¸¦é—œé–‰
            conn.commit()

            # é©—è­‰è³‡æ–™
            cursor.execute("SELECT COUNT(*) FROM inventory_data")
            count = cursor.fetchone()[0]
            conn.close()

            self.log_time(f"âœ… æ­£èˆªåº«å­˜è³‡æ–™è½‰æ›å®Œæˆï¼š{count} ç­†")
            return True

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›æ­£èˆªåº«å­˜è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return False

    def convert_procuretrack_data(self):
        """è½‰æ›æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ±è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›æ¡è³¼åˆ°è²¨é€²åº¦è³‡æ–™...")

        try:
            # è®€å– Excel æª”æ¡ˆ
            procure_xlsx_path = os.path.join(self.data_source_dir, "æ¡è³¼ç‹€æ³æ˜ç´°è¡¨.xlsx")
            if not os.path.exists(procure_xlsx_path):
                self.log_time(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{procure_xlsx_path}")
                return

            df_procure = pd.read_excel(procure_xlsx_path)
            self.log_time(f"ğŸ“Š è®€å–æ¡è³¼è³‡æ–™ï¼š{len(df_procure)} ç­†")

            # è½‰æ›æ—¥æœŸ
            # æ¬„ä½åç¨±å¯èƒ½è®Šå‹•ï¼Œé€™è£¡åˆ—å‡ºå¯èƒ½çš„æ—¥æœŸæ¬„ä½
            date_columns = [
                "æ¡è³¼æ—¥æœŸ",
                "é äº¤æ—¥æœŸ",
                "èˆ¹æœŸ",
                "é€²å€‰æ—¥æœŸ",
                "çµæ¡ˆæ—¥æœŸ",
                "æ—¥æœŸ",
                "å»ºç«‹æ—¥æœŸ",
            ]
            df_procure = self.convert_datetime_optimized(df_procure, date_columns)

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.procuretrack_db_path)

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            # å‡è¨­åªæœ‰ä¸€å¼µè¡¨ procure_data
            self.insert_data_in_chunks(
                df_procure, conn, "procure_data", chunk_size=1000
            )

            # å»ºç«‹ç´¢å¼•
            # å‡è¨­å¸¸ç”¨æŸ¥è©¢æ¬„ä½
            cursor = conn.cursor()
            procure_indexes = [
                ("æ¡è³¼å–®è™Ÿ", "idx_procure_po"),
                ("å» å•†åç¨±", "idx_procure_vendor"),
                ("ç”¢å“åç¨±", "idx_procure_product"),
            ]
            # æª¢æŸ¥é€™äº›æ¬„ä½æ˜¯å¦å­˜åœ¨å†å»ºç«‹ç´¢å¼•
            valid_indexes = []
            for col, idx_name in procure_indexes:
                if col in df_procure.columns:
                    valid_indexes.append((col, idx_name))

            self.create_indexes_batch(cursor, "procure_data", valid_indexes)

            conn.commit()
            conn.close()

            self.log_time(f"âœ… æ¡è³¼è³‡æ–™è½‰æ›å®Œæˆï¼š{len(df_procure)} ç­†")

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›æ¡è³¼è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_sales_ai_data(self):
        """è½‰æ› AI å°ˆç”¨éŠ·å”®è³‡æ–™ (åˆä½µç™¼è²¨ç‹€æ³åˆ†æè¡¨èˆ‡æ˜‡å³°éŠ·å”®è³‡æ–™)"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ› AI å°ˆç”¨éŠ·å”®è³‡æ–™ (sales_ai.db)...")

        try:
            # è®€å–éŠ·å”®è³‡æ–™æª”æ¡ˆ
            df_shipment_path = os.path.join(self.data_source_dir, "ç™¼è²¨ç‹€æ³åˆ†æè¡¨.xlsx")
            df_shengfeng_path = os.path.join(self.data_source_dir, "æ˜‡å³°éŠ·å”®è³‡æ–™.xlsx")

            df_list = []

            if os.path.exists(df_shipment_path):
                df_shipment = pd.read_excel(df_shipment_path)
                df_list.append(df_shipment)
                self.log_time(f"ğŸ“Š ç™¼è²¨ç‹€æ³åˆ†æè¡¨ï¼š{len(df_shipment)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shipment_path}")

            if os.path.exists(df_shengfeng_path):
                df_shengfeng = pd.read_excel(df_shengfeng_path)
                df_list.append(df_shengfeng)
                self.log_time(f"ğŸ“Š æ˜‡å³°éŠ·å”®è³‡æ–™ï¼š{len(df_shengfeng)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shengfeng_path}")

            if not df_list:
                self.log_time("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•éŠ·å”®è³‡æ–™æª”æ¡ˆ")
                return

            # è½‰æ›æ—¥æœŸæ™‚é–“
            # é€™è£¡éœ€è¦è™•ç†æ‰€æœ‰å¯èƒ½çš„æ—¥æœŸæ¬„ä½ï¼Œç¢ºä¿ AI èƒ½æ­£ç¢ºæŸ¥è©¢
            date_columns = ["ç™¼è²¨æ—¥æœŸ", "å–®æ“šæ—¥æœŸ", "å»ºç«‹æ—¥æœŸ", "éå¸³æ—¥æœŸ"]
            for i, df in enumerate(df_list):
                 # ä½¿ç”¨æ—¢æœ‰çš„æ—¥æœŸè½‰æ›é‚è¼¯
                if i == 0:  # ç™¼è²¨ç‹€æ³åˆ†æè¡¨
                     df_list[i] = self.convert_datetime_optimized(df, date_columns)
                else: # æ˜‡å³°éŠ·å”®è³‡æ–™ (é€šå¸¸æœ‰ç‰¹æ®Šæ—¥æœŸæ ¼å¼)
                     df_list[i] = self.convert_shengfeng_dates(df, date_columns)

            # åˆä½µè³‡æ–™ (concat æœƒè‡ªå‹•è™•ç†æ¬„ä½ä¸ä¸€è‡´ï¼Œç¼ºå°‘çš„æ¬„ä½è£œ NaN)
            df_combined = pd.concat(df_list, ignore_index=True)
            self.log_time(f"ğŸ“Š åˆä½µå¾Œç¸½ç­†æ•¸ï¼š{len(df_combined)}")

            # ç§»é™¤å‚™è¨»ä¸­çš„é›»è©±é€£å­—è™Ÿ (é¸ç”¨ï¼Œç‚ºäº†ä¿æŒèˆ‡ sales.db ä¸€è‡´)
            df_combined = self.normalize_remark_columns(df_combined, ["å‚™è¨»"])

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.sales_ai_db_path)
            cursor = conn.cursor()

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•ï¼Œå°‡æ‰€æœ‰è³‡æ–™å¯«å…¥å–®ä¸€è³‡æ–™è¡¨ 'sales_data'
            self.insert_data_in_chunks(df_combined, conn, "sales_data", chunk_size=1000)

            # å»ºç«‹ç´¢å¼• (é‡å°å¯èƒ½çš„å¸¸ç”¨æŸ¥è©¢æ¬„ä½)
            # å…ˆæª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
            potential_indexes = [
                ("å–®æ“šç·¨è™Ÿ", "idx_sales_ai_doc_no"),
                ("ç™¼è²¨æ—¥æœŸ", "idx_sales_ai_date"),
                ("å®¢æˆ¶åç¨±", "idx_sales_ai_customer"),
                ("ç”¢å“åç¨±", "idx_sales_ai_product"),
                ("æ¥­å‹™äººå“¡åç¨±", "idx_sales_ai_salesperson"),
                ("å®¢æˆ¶ä»£ç¢¼", "idx_sales_ai_customer_code"),
            ]
            
            valid_indexes = []
            for col, idx_name in potential_indexes:
                if col in df_combined.columns:
                    valid_indexes.append((col, idx_name))
            
            self.create_indexes_batch(cursor, "sales_data", valid_indexes)

            conn.commit()
            conn.close()

            self.log_time(f"âœ… AI éŠ·å”®è³‡æ–™è½‰æ›å®Œæˆï¼š{len(df_combined)} ç­† -> {self.sales_ai_db_path}")

        except Exception as e:
            self.log_time(f"âŒ è½‰æ› AI éŠ·å”®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_custody_data(self):
        """è½‰æ›å¯„å€‰è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›å¯„å€‰è³‡æ–™...")

        try:
            custody_xlsx_1 = os.path.join(self.data_source_dir, "å¯„å€‰è³‡æ–™.xlsx")
            custody_xlsx_2 = os.path.join(self.data_source_dir, "æ˜‡å³°å¯„åº«è³‡æ–™.xlsx")

            raw_frames = []
            if os.path.exists(custody_xlsx_1):
                df1 = pd.read_excel(custody_xlsx_1)
                raw_frames.append(df1)
                self.log_time(f"ğŸ“Š å¯„å€‰è³‡æ–™ï¼š{len(df1)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{custody_xlsx_1}")

            if os.path.exists(custody_xlsx_2):
                df2 = pd.read_excel(custody_xlsx_2)
                raw_frames.append(df2)
                self.log_time(f"ğŸ“Š æ˜‡å³°å¯„åº«è³‡æ–™ï¼š{len(df2)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{custody_xlsx_2}")

            if not raw_frames:
                self.log_time("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å¯„å€‰è³‡æ–™æª”æ¡ˆ")
                return

            date_columns = ["å–®æ“šæ—¥æœŸ"]
            processed_frames = []
            for idx, frame in enumerate(raw_frames):
                frame = self.normalize_phone_columns(frame.copy())
                if idx == 0:
                    processed_frames.append(
                        self.convert_datetime_optimized(frame, date_columns)
                    )
                else:
                    processed_frames.append(
                        self.convert_shengfeng_dates(frame, date_columns)
                    )

            df = pd.concat(processed_frames, ignore_index=True)
            df = self.normalize_remark_columns(df, ["å‚™è¨»"])
            self.log_time(f"ğŸ“Š åˆä½µå¾Œç¸½ç­†æ•¸ï¼š{len(df)}")
            # ä¸»æª”æ¬„ä½å°æ‡‰
            main_columns_mapping = {
                "å–®æ“šç·¨è™Ÿ": "å–®æ“šç·¨è™Ÿ",
                "å–®æ“šæ—¥æœŸ": "å–®æ“šæ—¥æœŸ",
                "å€Ÿè²¨å°è±¡": "å®¢æˆ¶ä»£ç¢¼",
                "å€Ÿè²¨å°è±¡åç¨±": "å®¢æˆ¶åç¨±",
                "æ¥­å‹™äººå“¡åç¨±": "æ¥­å‹™äººå“¡åç¨±",
                "è¯çµ¡é›»è©±": "è¯çµ¡é›»è©±",
                "é€è²¨åœ°å€": "é€è²¨åœ°å€",
                "å‚™è¨»": "å‚™è¨»",
            }

            available_main_columns = {
                k: v for k, v in main_columns_mapping.items() if k in df.columns
            }
            # è¦æ•´ä¾†æºåç¨±æ¬„ä½ï¼Œè®“æ˜ å°„èƒ½ç”¢å‡ºå®Œæ•´çš„ã€Œå®¢æˆ¶åç¨±ã€
            try:
                if "å®¢æˆ¶åç¨±" in df.columns:
                    df["å®¢æˆ¶åç¨±"] = (
                        df["å®¢æˆ¶åç¨±"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å¯„è²¨å°è±¡åç¨±" in df.columns:
                    df["å¯„è²¨å°è±¡åç¨±"] = (
                        df["å¯„è²¨å°è±¡åç¨±"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å®¢æˆ¶" in df.columns:
                    df["å®¢æˆ¶"] = (
                        df["å®¢æˆ¶"].astype(str)
                        .replace("nan", "")
                        .replace("None", "")
                        .str.strip()
                    )
                if "å¯„è²¨å°è±¡åç¨±" not in df.columns and "å®¢æˆ¶" in df.columns:
                    df["å¯„è²¨å°è±¡åç¨±"] = df["å®¢æˆ¶"]
                elif "å¯„è²¨å°è±¡åç¨±" in df.columns and "å®¢æˆ¶" in df.columns:
                    __mask_empty_cus = df["å¯„è²¨å°è±¡åç¨±"].isin(["", "nan", "None"]) | df["å¯„è²¨å°è±¡åç¨±"].isna()
                    df.loc[__mask_empty_cus, "å¯„è²¨å°è±¡åç¨±"] = df.loc[__mask_empty_cus, "å®¢æˆ¶"].fillna("")
                if "å¯„è²¨å°è±¡åç¨±" not in df.columns and "å®¢æˆ¶åç¨±" in df.columns:
                    df["å¯„è²¨å°è±¡åç¨±"] = df["å®¢æˆ¶åç¨±"]
                elif "å¯„è²¨å°è±¡åç¨±" in df.columns and "å®¢æˆ¶åç¨±" in df.columns:
                    __mask_empty_cus2 = df["å¯„è²¨å°è±¡åç¨±"].isin(["", "nan", "None"]) | df["å¯„è²¨å°è±¡åç¨±"].isna()
                    df.loc[__mask_empty_cus2, "å¯„è²¨å°è±¡åç¨±"] = df.loc[__mask_empty_cus2, "å®¢æˆ¶åç¨±"].fillna("")
            except Exception:
                pass

            df_main = df[list(available_main_columns.keys())].rename(
                columns=available_main_columns
            )
            # ä¾å–®æ“šç·¨è™Ÿæ’åºï¼Œè®“æœ‰ã€Œå®¢æˆ¶åç¨±ã€çš„ç´€éŒ„å„ªå…ˆ
            try:
                if "å–®æ“šç·¨è™Ÿ" in df_main.columns and "å®¢æˆ¶åç¨±" in df_main.columns:
                    df_main["__has_name__"] = df_main["å®¢æˆ¶åç¨±"].astype(str).str.strip().ne("")
                    df_main.sort_values(["å–®æ“šç·¨è™Ÿ", "__has_name__"], ascending=[True, False], inplace=True)
                    if "__has_name__" in df_main.columns:
                        df_main.drop(columns=["__has_name__"], inplace=True)
            except Exception:
                pass
            df_main = df_main.drop_duplicates(subset=["å–®æ“šç·¨è™Ÿ"])

            # è™•ç†å®¢æˆ¶ä»£ç¢¼æ ¼å¼
            if "å®¢æˆ¶ä»£ç¢¼" in df_main.columns:
                df_main["å®¢æˆ¶ä»£ç¢¼"] = (
                    df_main["å®¢æˆ¶ä»£ç¢¼"]
                    .astype(str)
                    .replace("nan", "")
                    .replace("None", "")
                )

            # æ˜ç´°æ¬„ä½å°æ‡‰
            detail_columns_mapping = {
                "å–®æ“šç·¨è™Ÿ": "å–®æ“šç·¨è™Ÿ",
                "ç”¢å“åç¨±": "ç”¢å“åç¨±",
                "å€‰åº«ç¢ºèªæ•¸é‡": "äº¤æ˜“æ•¸é‡",
            }

            available_detail_columns = {
                k: v for k, v in detail_columns_mapping.items() if k in df.columns
            }
            df_detail = df[list(available_detail_columns.keys())].rename(
                columns=available_detail_columns
            )

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.custody_db_path)

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            self.insert_data_in_chunks(df_main, conn, "custody_main", chunk_size=1000)
            self.insert_data_in_chunks(
                df_detail, conn, "custody_detail", chunk_size=1000
            )

            # å»ºç«‹ç´¢å¼•
            cursor = conn.cursor()
            custody_main_indexes = [
                ("å–®æ“šç·¨è™Ÿ", "idx_custody_main_doc_no"),
                ("å®¢æˆ¶åç¨±", "idx_custody_main_customer"),
                ("é€è²¨åœ°å€", "idx_custody_main_address"),
                ("è¯çµ¡é›»è©±", "idx_custody_main_phone"),
                ("å‚™è¨»", "idx_custody_main_remark"),
            ]
            self.create_indexes_batch(cursor, "custody_main", custody_main_indexes)

            custody_detail_indexes = [("å–®æ“šç·¨è™Ÿ", "idx_custody_detail_doc_no")]
            self.create_indexes_batch(cursor, "custody_detail", custody_detail_indexes)

            conn.commit()
            conn.close()

            self.log_time(
                f"âœ… å¯„å€‰è³‡æ–™è½‰æ›å®Œæˆï¼šä¸»æª” {len(df_main)} ç­†ï¼Œæ˜ç´° {len(df_detail)} ç­†"
            )

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›å¯„å€‰è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_service_card_data(self):
        """è½‰æ›æœå‹™ç™»è¨˜å¡è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›æœå‹™ç™»è¨˜å¡è³‡æ–™...")

        try:
            # è®€å–æœå‹™ç™»è¨˜å¡æª”æ¡ˆï¼ˆå¾çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„ï¼‰
            service_card_path = os.path.join(self.data_source_dir, "æœå‹™ç™»è¨˜å¡.xlsx")

            if not os.path.exists(service_card_path):
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{service_card_path}")
                return

            # è®€å–Excelæª”æ¡ˆï¼Œè·³éå‰å…©è¡Œï¼ˆæ¨™é¡Œè¡Œï¼‰
            df = pd.read_excel(service_card_path, skiprows=2)
            self.log_time(f"ğŸ“Š æœå‹™ç™»è¨˜å¡åŸå§‹è³‡æ–™ï¼š{len(df)} ç­†")

            # éæ¿¾æ‰ç©ºç™½è¡Œ
            df = df.dropna(how="all")
            self.log_time(f"ğŸ“Š æœå‹™ç™»è¨˜å¡æœ‰æ•ˆè³‡æ–™ï¼š{len(df)} ç­†")

            df = self.normalize_phone_columns(df)

            # é‡æ–°å‘½åæ¬„ä½ï¼ˆæ ¹æ“šå¯¦éš›çš„Excelçµæ§‹ï¼‰
            if len(df.columns) >= 18:
                column_mapping = {
                    df.columns[0]: "æœå‹™ç™»è¨˜è™Ÿ",
                    df.columns[1]: "æ ¸ç®—çµ„ç¹”",
                    df.columns[2]: "å®¢æˆ¶ä»£ç¢¼",
                    df.columns[3]: "å®¢æˆ¶åç¨±",
                    df.columns[4]: "è¯çµ¡äºº",
                    df.columns[5]: "è¯çµ¡é›»è©±",
                    df.columns[6]: "æœå‹™åœ°å€",
                    df.columns[7]: "ç”¢å“å‹è™Ÿ",
                    df.columns[8]: "ç”¢å“åºè™Ÿ",
                    df.columns[9]: "æœå‹™é …ç›®",
                    df.columns[10]: "æœå‹™äººå“¡",
                    df.columns[11]: "ç™»è¨˜æ—¥æœŸ",
                    df.columns[12]: "æœå‹™æ—¥æœŸ",
                    df.columns[13]: "å®Œæˆæ—¥æœŸ",
                    df.columns[14]: "æœå‹™ç‹€æ…‹",
                    df.columns[15]: "å‚™è¨»",
                    df.columns[16]: "ä¿å›ºæœŸé™",
                    df.columns[17]: "è£æ©Ÿä½ç½®èªªæ˜",
                }
                df = df.rename(columns=column_mapping)

                df = self.normalize_remark_columns(df, ["å‚™è¨»"])

            # è½‰æ›æ—¥æœŸæ™‚é–“
            date_columns = ["ç™»è¨˜æ—¥æœŸ", "æœå‹™æ—¥æœŸ", "å®Œæˆæ—¥æœŸ", "ä¿å›ºæœŸé™"]
            df = self.convert_datetime_optimized(df, date_columns)

            # è™•ç†å®¢æˆ¶ä»£ç¢¼æ¬„ä½ï¼ˆé¿å…æµ®é»æ•¸é¡¯ç¤ºï¼‰
            if "å®¢æˆ¶ä»£ç¢¼" in df.columns:
                df["å®¢æˆ¶ä»£ç¢¼"] = (
                    df["å®¢æˆ¶ä»£ç¢¼"].astype(str).replace("nan", "").replace("None", "")
                )
                # å»é™¤æµ®é»æ•¸çš„å°æ•¸éƒ¨åˆ†
                df["å®¢æˆ¶ä»£ç¢¼"] = df["å®¢æˆ¶ä»£ç¢¼"].apply(
                    lambda x: x.split(".")[0] if "." in str(x) else x
                )

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.service_card_db_path)

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
            self.insert_data_in_chunks(df, conn, "service_card_data", chunk_size=1000)

            # å»ºç«‹ç´¢å¼•ï¼ˆä½¿ç”¨å¯¦éš›çš„æ¬„ä½åç¨±ï¼‰
            cursor = conn.cursor()
            service_card_indexes = [
                ("æœå‹™ç™»è¨˜è™Ÿ", "idx_service_card_no"),
                ("å®¢æˆ¶åç¨±", "idx_service_card_customer"),
                ("å®¢æˆ¶ä»£ç¢¼", "idx_service_card_customer_code"),
                ("æœå‹™åœ°å€", "idx_service_card_address"),
                ("è¯çµ¡é›»è©±", "idx_service_card_phone"),
                ("æœå‹™é …ç›®", "idx_service_card_service"),
                ("æœå‹™äººå“¡", "idx_service_card_staff"),
                ("ç™»è¨˜æ—¥æœŸ", "idx_service_card_reg_date"),
                ("æœå‹™æ—¥æœŸ", "idx_service_card_service_date"),
                ("ç”¢å“å‹è™Ÿ", "idx_service_card_model"),
                ("ç”¢å“åºè™Ÿ", "idx_service_card_serial"),
            ]
            self.create_indexes_batch(cursor, "service_card_data", service_card_indexes)

            # è®€å–æœå‹™ç™»è¨˜å¡ ç¬¬äºŒé ï¼ˆæ˜ç´°ï¼‰ï¼ŒåŒ¯å…¥ service_card_detail
            try:
                df_detail_raw = pd.read_excel(service_card_path, sheet_name=1, skiprows=2)
                # è‹¥åµæ¸¬åˆ° Unnamed æ¬„ä½ï¼Œä»£è¡¨è¡¨é ­è·¨è¡Œï¼Œæ”¹ç”¨ header=None å¾Œç¬¬2åˆ—ç•¶è¡¨é ­
                if any(str(c).startswith("Unnamed") for c in df_detail_raw.columns):
                    df_tmp = pd.read_excel(service_card_path, sheet_name=1, skiprows=2, header=None)
                    if len(df_tmp) >= 2:
                        header = [str(x).strip() if pd.notna(x) else "" for x in df_tmp.iloc[1].tolist()]
                        df_detail = df_tmp.iloc[2:].copy()
                        df_detail.columns = header
                    else:
                        df_detail = df_detail_raw
                else:
                    df_detail = df_detail_raw

                # å»é™¤å…¨ç©ºåˆ—
                before_detail = len(df_detail)
                df_detail = df_detail.dropna(how="all")
                self.log_time(f"ğŸ“Š æœå‹™ç™»è¨˜å¡-æ˜ç´° åŸå§‹ç­†æ•¸: {before_detail} -> æ¸…ç†å¾Œ: {len(df_detail)}")

                # æ¬„ä½åç¨±å°æ‡‰ï¼šå°‡å¸¸è¦‹ç•°åå°æ‡‰åˆ°ç³»çµ±æ¬„ä½
                wanted_map = {
                    "æœå‹™ç™»è¨˜è™Ÿ": ["æœå‹™ç™»è¨˜è™Ÿ", "æœå‹™ç™»è¨˜å¡è™Ÿ", "ç™»è¨˜è™Ÿ", "Aç™»è¨˜è™Ÿ"],
                    "ç‰©æ–™ä»£ç¢¼": ["ç‰©æ–™ä»£ç¢¼", "å“è™Ÿ", "æ–™è™Ÿ"],
                    "ç”¢å“åç¨±": ["ç”¢å“åç¨±", "å“å"],
                    "æ¨™æº–å”®åƒ¹": ["æ¨™æº–å”®åƒ¹", "å”®åƒ¹", "å–®åƒ¹"],
                    "æ›´æ›æœŸé™_æœˆ": ["æ›´æ›æœŸé™_æœˆ", "æ›´æ›æœŸé™(æœˆ)", "æ›´æ›æœŸé™"],
                    "ä¸Šæ¬¡æ›´æ›": ["ä¸Šæ¬¡æ›´æ›"],
                    "ä¸Šæ¬¡é€šçŸ¥": ["ä¸Šæ¬¡é€šçŸ¥"],
                    "ä¸‹æ¬¡é€šçŸ¥": ["ä¸‹æ¬¡é€šçŸ¥"],
                    "æ•¸é‡": ["æ•¸é‡", "Qty"],
                }
                rename_map = {}
                for std, candidates in wanted_map.items():
                    for c in candidates:
                        if c in df_detail.columns:
                            rename_map[c] = std
                            break
                if rename_map:
                    df_detail = df_detail.rename(columns=rename_map)

                # æ¨™æº–åŒ–æ—¥æœŸæ¬„ä½ï¼ˆè‹¥å­˜åœ¨ï¼‰
                date_cols = [c for c in ["ä¸Šæ¬¡æ›´æ›", "ä¸Šæ¬¡é€šçŸ¥", "ä¸‹æ¬¡é€šçŸ¥"] if c in df_detail.columns]
                if date_cols:
                    df_detail = self.convert_datetime_optimized(df_detail, date_cols)
                    # è½‰æˆ å¹´/æœˆ/æ—¥ï¼ˆä¸å«æ™‚åˆ†ç§’ï¼‰
                    for _col in date_cols:
                        try:
                            s = pd.to_datetime(df_detail[_col], errors="coerce")
                            out = s.dt.strftime("%Y/%m/%d")
                            # å°‡ NaT è½‰ç‚ºç©ºå­—ä¸²ï¼Œé¿å…å‡ºç¾ 'NaT'
                            df_detail[_col] = out.where(s.notna(), "")
                        except Exception:
                            pass

                # åƒ…ä¿ç•™ç³»çµ±ä½¿ç”¨æ¬„ä½
                keep_cols = [
                    "æœå‹™ç™»è¨˜è™Ÿ",
                    "ç‰©æ–™ä»£ç¢¼",
                    "ç”¢å“åç¨±",
                    "æ¨™æº–å”®åƒ¹",
                    "æ›´æ›æœŸé™_æœˆ",
                    "ä¸Šæ¬¡æ›´æ›",
                    "ä¸Šæ¬¡é€šçŸ¥",
                    "ä¸‹æ¬¡é€šçŸ¥",
                    "æ•¸é‡",
                ]
                if "æœå‹™ç™»è¨˜è™Ÿ" not in df_detail.columns:
                    self.log_time("âŒ ç„¡æ³•æ‰¾åˆ°æ˜ç´°å¿…è¦æ¬„ä½ã€æœå‹™ç™»è¨˜è™Ÿã€ï¼Œå·²ç•¥éæ˜ç´°åŒ¯å…¥ã€‚")
                else:
                    existing = [c for c in keep_cols if c in df_detail.columns]
                    df_detail = df_detail[existing].copy()

                    # é‡æ–°å»ºç«‹ service_card_detailï¼ˆå›ºå®šçµæ§‹ï¼ŒåŒ…å«è‡ªå¢ idï¼‰
                    cursor.execute("DROP TABLE IF EXISTS service_card_detail")
                    cursor.execute(
                        """
                        CREATE TABLE service_card_detail (
                          id INTEGER PRIMARY KEY AUTOINCREMENT,
                          "æœå‹™ç™»è¨˜è™Ÿ" TEXT,
                          "ç‰©æ–™ä»£ç¢¼" TEXT,
                          "ç”¢å“åç¨±" TEXT,
                          "æ¨™æº–å”®åƒ¹" TEXT,
                          "æ›´æ›æœŸé™_æœˆ" TEXT,
                          "ä¸Šæ¬¡æ›´æ›" TEXT,
                          "ä¸Šæ¬¡é€šçŸ¥" TEXT,
                          "ä¸‹æ¬¡é€šçŸ¥" TEXT,
                          "æ•¸é‡" TEXT
                        )
                        """
                    )

                    # æ¬„ä½è£œé½Šé †åº
                    ordered_cols = [
                        "æœå‹™ç™»è¨˜è™Ÿ",
                        "ç‰©æ–™ä»£ç¢¼",
                        "ç”¢å“åç¨±",
                        "æ¨™æº–å”®åƒ¹",
                        "æ›´æ›æœŸé™_æœˆ",
                        "ä¸Šæ¬¡æ›´æ›",
                        "ä¸Šæ¬¡é€šçŸ¥",
                        "ä¸‹æ¬¡é€šçŸ¥",
                        "æ•¸é‡",
                    ]
                    for col in ordered_cols:
                        if col not in df_detail.columns:
                            df_detail[col] = ""
                    df_detail = df_detail[ordered_cols]

                    # æ‰¹æ¬¡å¯«å…¥ï¼ˆé¿å… pandas é‡å»ºè¡¨çµæ§‹ï¼‰
                    insert_sql = (
                        "INSERT INTO service_card_detail(\"æœå‹™ç™»è¨˜è™Ÿ\",\"ç‰©æ–™ä»£ç¢¼\",\"ç”¢å“åç¨±\",\"æ¨™æº–å”®åƒ¹\",\"æ›´æ›æœŸé™_æœˆ\",\"ä¸Šæ¬¡æ›´æ›\",\"ä¸Šæ¬¡é€šçŸ¥\",\"ä¸‹æ¬¡é€šçŸ¥\",\"æ•¸é‡\") "
                        "VALUES (?,?,?,?,?,?,?,?,?)"
                    )
                    data = [tuple("" if x is None else x for x in row) for row in df_detail.itertuples(index=False, name=None)]
                    chunk_size = 1000
                    for i in range(0, len(data), chunk_size):
                        cursor.executemany(insert_sql, data[i : i + chunk_size])

                    # ç´¢å¼•
                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_svc_detail_card_no ON service_card_detail(\"æœå‹™ç™»è¨˜è™Ÿ\")")
                    cursor.execute("CREATE INDEX IF NOT EXISTS idx_svc_detail_product ON service_card_detail(\"ç”¢å“åç¨±\")")

                    self.log_time(f"âœ… æœå‹™ç™»è¨˜å¡-æ˜ç´° å·²åŒ¯å…¥ç­†æ•¸: {len(df_detail)}")
            except Exception as de:
                self.log_time(f"âŒ åŒ¯å…¥æœå‹™ç™»è¨˜å¡æ˜ç´°æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(de)}")

            conn.commit()
            conn.close()

            self.log_time(f"âœ… æœå‹™ç™»è¨˜å¡è³‡æ–™è½‰æ›å®Œæˆï¼š{len(df)} ç­†")

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›æœå‹™ç™»è¨˜å¡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    def convert_crm_notes(self):
        """è½‰æ› CRMè¨˜äº‹.xlsx ç‚º SQLite"""
        self.log_time("?? é–‹å§‹è½‰æ› CRMè¨˜äº‹.xlsx ...")

        try:
            if not os.path.exists(self.crm_source_path):
                self.log_time(f"?? æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{self.crm_source_path}")
                return False

            os.makedirs(self.crm_dir, exist_ok=True)

            df = pd.read_excel(self.crm_source_path)
            self.log_time(f"?? CRMè¨˜äº‹åŸå§‹åˆ—æ•¸ï¼š{len(df)} ç­†")

            normalized_columns = []
            for idx, col in enumerate(df.columns):
                if pd.isna(col) or str(col).strip() == "":
                    normalized_columns.append(f"æ¬„ä½_{idx + 1}")
                else:
                    normalized_columns.append(str(col).strip())
            df.columns = normalized_columns

            before_drop = len(df)
            df = df.dropna(how="all")
            self.log_time(f"?? æ¸…ç†ç©ºç™½åˆ—å¾Œï¼š{before_drop} ç­† -> {len(df)} ç­†")

            conn = sqlite3.connect(self.crm_db_path)
            df.to_sql('crm_notes', conn, if_exists='replace', index=False)
            conn.close()

            self.log_time(f"? CRMè¨˜äº‹å¯«å…¥ SQLite å®Œæˆï¼Œå…± {len(df)} ç­†")
            return True

        except Exception as e:
            self.log_time(f"? è½‰æ› CRMè¨˜äº‹.xlsx ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return False
    def convert_inventory_inquiry_data(self):
        """è½‰æ›æ­£èˆªåº«å­˜è³‡æ–™"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ›æ­£èˆªåº«å­˜è³‡æ–™...")

        try:
            # è®€å–Excelæª”æ¡ˆï¼ˆå¾çµ±ä¸€è³‡æ–™ä¾†æºç›®éŒ„ï¼‰
            excel_path = os.path.join(self.data_source_dir, "æ­£èˆªåº«å­˜è³‡æ–™.xlsx")
            if not os.path.exists(excel_path):
                self.log_time(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{excel_path}")
                return False

            df = pd.read_excel(excel_path)
            self.log_time(f"ğŸ“Š è®€å–åˆ° {len(df)} ç­†åº«å­˜è³‡æ–™")

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.inventory_data_db_path)
            cursor = conn.cursor()

            # åˆªé™¤èˆŠè¡¨æ ¼ä¸¦é‡æ–°å»ºç«‹
            cursor.execute("DROP TABLE IF EXISTS inventory_data")

            # å»ºç«‹åº«å­˜è³‡æ–™è¡¨ï¼ˆå¯¦éš›æ’å…¥æ™‚æœƒç”± DataFrame é€é to_sql é‡å»ºï¼Œæ­¤çµæ§‹åƒ…ä¾›åƒè€ƒï¼‰
            cursor.execute(
                """
                CREATE TABLE inventory_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    product_name TEXT,
                    warehouse_name TEXT,
                    warehouse_partner_name TEXT,
                    inventory_type TEXT,
                    product_code TEXT,
                    specification TEXT,
                    unit TEXT,
                    quantity REAL,
                    unit_price REAL,
                    total_amount REAL,
                    last_update_date TEXT,
                    remark TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # è™•ç†è³‡æ–™æ¬„ä½å°æ‡‰ï¼ˆæ ¹æ“šå¯¦éš›Excelçµæ§‹èª¿æ•´ï¼‰
            # å…ˆæª¢æŸ¥Excelæª”æ¡ˆçš„æ¬„ä½çµæ§‹
            self.log_time(f"ğŸ“‹ Excelæ¬„ä½ï¼š{list(df.columns)}")

            # å¦‚æœExcelæª”æ¡ˆæœ‰è³‡æ–™ï¼Œé€²è¡Œè½‰æ›
            if len(df) > 0:
                # å»ºç«‹æ¬„ä½å°æ‡‰å­—å…¸
                column_mapping = {}
                for col in df.columns:
                    if "ç”¢å“åç¨±" in col:
                        column_mapping[col] = "product_name"
                    elif "å€‰åº«åç¨±" in col:
                        column_mapping[col] = "warehouse_name"
                    elif "å€‰åº«å¾€ä¾†å°è±¡åç¨±" in col or "å¾€ä¾†å°è±¡" in col:
                        column_mapping[col] = "warehouse_partner_name"
                    elif "å­˜è²¨å±¬æ€§" in col or "åº«å­˜å±¬æ€§" in col or col == "å±¬æ€§":
                        # ç›´æ¥æ²¿ç”¨ Excel çš„å­˜è²¨å±¬æ€§ï¼Œä¸åšé‚è¼¯æ›ç®—
                        column_mapping[col] = "inventory_type"
                    elif "ç”¢å“ä»£ç¢¼" in col or "ä»£ç¢¼" in col:
                        column_mapping[col] = "product_code"
                    elif "è¦æ ¼" in col:
                        column_mapping[col] = "specification"
                    elif "å–®ä½" in col:
                        column_mapping[col] = "unit"
                    elif "æ•¸é‡" in col:
                        column_mapping[col] = "quantity"
                    elif "å–®åƒ¹" in col:
                        column_mapping[col] = "unit_price"
                    elif "ç¸½é‡‘é¡" in col or "é‡‘é¡" in col:
                        column_mapping[col] = "total_amount"
                    elif "æ›´æ–°æ—¥æœŸ" in col or "æ—¥æœŸ" in col:
                        column_mapping[col] = "last_update_date"
                    elif "å‚™è¨»" in col:
                        column_mapping[col] = "remark"

                # é‡æ–°å‘½åæ¬„ä½
                df_mapped = df.rename(columns=column_mapping)

                # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å¡«å…¥ç©ºå€¼
                required_columns = [
                    "product_name",
                    "warehouse_name",
                    "warehouse_partner_name",
                    "inventory_type",
                    "product_code",
                    "specification",
                    "unit",
                    "quantity",
                    "unit_price",
                    "total_amount",
                    "last_update_date",
                    "remark",
                ]

                for col in required_columns:
                    if col not in df_mapped.columns:
                        df_mapped[col] = ""

                # åªä¿ç•™éœ€è¦çš„æ¬„ä½
                df_final = df_mapped[required_columns]

                df_final = self.normalize_remark_columns(df_final, ["remark"])
                # å­˜è²¨å±¬æ€§æ¬„ä½æ¨™æº–åŒ– + é¡¯ç¤ºç”¨æ–‡å­—å°æ‡‰
                # è‡ªæœ‰åº«å­˜ -> ä¸–ç£Šã€å€Ÿå…¥åº«å­˜ -> å¯„å€‰ã€å€Ÿå‡ºåº«å­˜ -> å€Ÿå‡º
                if "inventory_type" in df_final.columns:
                    inv = df_final["inventory_type"].fillna("").astype(str).str.strip()
                    mapping = {
                        "è‡ªæœ‰åº«å­˜": "ä¸–ç£Š",
                        "å€Ÿå…¥åº«å­˜": "å¯„å€‰",
                        "å€Ÿå‡ºåº«å­˜": "å€Ÿå‡º",
                        # ä¹Ÿæ”¯æ´å¯èƒ½çš„ç°¡å¯«
                        "è‡ªæœ‰": "ä¸–ç£Š",
                        "å€Ÿå…¥": "å¯„å€‰",
                        "å€Ÿå‡º": "å€Ÿå‡º",
                    }
                    df_final["inventory_type"] = inv.replace(mapping)

                # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•
                self.insert_data_in_chunks(
                    df_final, conn, "inventory_data", chunk_size=1000
                )

                # å»ºç«‹ç´¢å¼•ï¼ˆé‡å°æŸ¥è©¢éœ€æ±‚ï¼‰
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_product_name ON inventory_data(product_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_warehouse_name ON inventory_data(warehouse_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_warehouse_partner_name ON inventory_data(warehouse_partner_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_inventory_type ON inventory_data(inventory_type)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_product_code ON inventory_data(product_code)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_quantity ON inventory_data(quantity)"
                )

                # å»ºç«‹è¤‡åˆç´¢å¼•ï¼ˆæå‡å¤šæ¬„ä½æŸ¥è©¢æ•ˆèƒ½ï¼‰
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_product_warehouse ON inventory_data(product_name, warehouse_name)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_warehouse_partner ON inventory_data(warehouse_name, warehouse_partner_name)"
                )

            # æäº¤ä¸¦é—œé–‰
            conn.commit()

            # é©—è­‰è³‡æ–™
            cursor.execute("SELECT COUNT(*) FROM inventory_data")
            count = cursor.fetchone()[0]
            conn.close()

            self.log_time(f"âœ… æ­£èˆªåº«å­˜è³‡æ–™è½‰æ›å®Œæˆï¼š{count} ç­†")
            return True

        except Exception as e:
            self.log_time(f"âŒ è½‰æ›æ­£èˆªåº«å­˜è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return False

    def convert_procuretrack_data(self):
        """åŒ¯å…¥æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤è³‡æ–™ï¼Œæ¡æ›´æ–°æˆ–æ–°å¢(UPSERT)é‚è¼¯ï¼Œä¸¦ä¿ç•™æ‰‹å‹•ç¶­è­·æ¬„ä½ã€‚"""
        self.log_time("ğŸ”„ é–‹å§‹åŒ¯å…¥æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤è³‡æ–™...")

        try:
            excel_path = os.path.join(self.data_source_dir, "æ¡è³¼ç‹€æ³æ˜ç´°è¡¨.xlsx")
            if not os.path.exists(excel_path):
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{excel_path}")
                return False

            df = pd.read_excel(excel_path)
            if "åºè™Ÿ" not in df.columns:
                self.log_time("âš ï¸ Excel ç¼ºå°‘ã€Œåºè™Ÿã€æ¬„ä½ï¼Œç„¡æ³•åŸ·è¡Œæ–°çš„åŒ¯å…¥é‚è¼¯ï¼Œåœæ­¢åŒ¯å…¥ã€‚")
                return False
            
            self.log_time(f"ğŸ“Š è®€å– Excel è³‡æ–™ç¸½ç­†æ•¸ï¼š{len(df)}")

            os.makedirs(self.procuretrack_db_dir, exist_ok=True)
            conn = sqlite3.connect(self.procuretrack_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # æª¢æŸ¥ä¸¦æ›´æ–°è³‡æ–™åº«çµæ§‹
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS procure_items (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    po_number TEXT,
                    item_serial_number TEXT,
                    product_code TEXT,
                    product_name TEXT,
                    quantity INTEGER,
                    warehouse_qty INTEGER,
                    delivery_date TEXT,
                    dispatch_date TEXT,
                    warehouse TEXT,
                    arrival_date TEXT,
                    ship_info TEXT,
                    status TEXT,
                    goods_status TEXT,
                    UNIQUE(po_number, item_serial_number)
                )
                """
            )
            conn.commit()

            try:
                cols = {row[1] for row in cursor.execute("PRAGMA table_info(procure_items)").fetchall()}
                if "item_serial_number" not in cols:
                    cursor.execute("ALTER TABLE procure_items ADD COLUMN item_serial_number TEXT")
                if "goods_status" not in cols:
                    cursor.execute("ALTER TABLE procure_items ADD COLUMN goods_status TEXT")
                for col_to_check in ["product_code", "warehouse_qty", "warehouse", "dispatch_date"]:
                     if col_to_check not in cols:
                        cursor.execute(f"ALTER TABLE procure_items ADD COLUMN {col_to_check} TEXT")
                conn.commit()
            except Exception as e:
                self.log_time(f"âš ï¸ æª¢æŸ¥/æ–°å¢æ¬„ä½æ™‚ç™¼ç”ŸéŒ¯èª¤ (å¯å¿½ç•¥): {e}")

            updated_count = 0
            inserted_count = 0
            skipped_count = 0
            errors_count = 0

            def _normalize_delivery(val):
                if pd.isna(val): return ""
                try: s = str(val).strip()
                except Exception: return ""
                if not s: return ""
                import re
                s = re.sub(r"\([^)]*\)", "", s).strip()
                try:
                    dt = pd.to_datetime(s, errors="coerce")
                    return dt.date().isoformat() if pd.notna(dt) else s
                except Exception: return s

            for index, row in df.iterrows():
                try:
                    po_number = str(row.get("å–®æ“šç·¨è™Ÿ") or "").strip()
                    item_serial_number = str(row.get("åºè™Ÿ") or "").strip()

                    self.log_time(f"--- [è™•ç†ä¸­] Excel ç¬¬ {index + 2} è¡Œ: å–®è™Ÿ={po_number}, åºè™Ÿ={item_serial_number} ---")

                    if not po_number or not item_serial_number:
                        self.log_time("   -> [è·³é] å–®è™Ÿæˆ–åºè™Ÿç‚ºç©ºã€‚")
                        skipped_count += 1
                        continue
                    
                    product_code = str(row.get("ç”¢å“ä»£ç¢¼") or "").strip()
                    product_name = str(row.get("ç”¢å“åç¨±") or "").strip()
                    quantity_raw = row.get("äº¤æ˜“æ•¸é‡")
                    warehouse_qty_raw = row.get("å€‰åº«ç¢ºèªæ•¸é‡")
                    warehouse_name = str(row.get("å€‰åº«") or "").strip()
                    delivery_date = _normalize_delivery(row.get("äº¤è²¨æ—¥æœŸ"))
                    status_val = str(row.get("ç‹€æ…‹") or "").strip()
                    self.log_time(f"   [è®€å–å€¼] Excelç‹€æ…‹: '{status_val}', æ•¸é‡: {quantity_raw}")

                    try: quantity = int(quantity_raw) if pd.notna(quantity_raw) else 0
                    except (ValueError, TypeError): quantity = 0
                    try: warehouse_qty = int(warehouse_qty_raw) if pd.notna(warehouse_qty_raw) else 0
                    except (ValueError, TypeError): warehouse_qty = 0

                    cursor.execute(
                        "SELECT id FROM procure_items WHERE po_number = ? AND item_serial_number = ?",
                        (po_number, item_serial_number),
                    )
                    existing_record = cursor.fetchone()

                    if existing_record:
                        self.log_time(f"   -> [åˆ¤æ–·] æ‰¾ä¸åˆ°å°æ‡‰ç´€éŒ„ (ID: {existing_record['id']})ï¼Œæº–å‚™åŸ·è¡Œã€æ›´æ–°ã€‘ã€‚")
                        self.log_time(f"      [æ›´æ–°å€¼] status='{status_val}'")
                        cursor.execute(
                            """
                            UPDATE procure_items
                            SET
                                product_code = ?, product_name = ?, quantity = ?,
                                warehouse_qty = ?, delivery_date = ?, warehouse = ?,
                                status = ?
                            WHERE
                                id = ?
                            """,
                            (
                                product_code, product_name, quantity,
                                warehouse_qty, delivery_date, warehouse_name,
                                status_val,
                                existing_record["id"],
                            ),
                        )
                        updated_count += 1
                    else:
                        self.log_time("   -> [åˆ¤æ–·] æ‰¾ä¸åˆ°å°æ‡‰ç´€éŒ„ï¼Œæº–å‚™åŸ·è¡Œã€æ–°å¢ã€‘ã€‚")
                        cursor.execute(
                            """
                            INSERT INTO procure_items (
                                po_number, item_serial_number, product_code, product_name, quantity, warehouse_qty,
                                delivery_date, warehouse, status,
                                dispatch_date, arrival_date, ship_info, goods_status
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """,
                            (
                                po_number, item_serial_number, product_code, product_name, quantity, warehouse_qty,
                                delivery_date, warehouse_name, status_val,
                                "", "", "", ""
                            ),
                        )
                        inserted_count += 1

                except Exception as e:
                    errors_count += 1
                    self.log_time(f"   -> XXX [éŒ¯èª¤] è™•ç†æ­¤è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}")

            conn.commit()
            conn.close()

            self.log_time("-" * 40)
            self.log_time(
                f"ğŸ“Š æ¡è³¼åˆ°è²¨é€²åº¦åŒ¯å…¥å®Œæˆï¼šæ–°å¢ {inserted_count} ç­†ï¼Œæ›´æ–° {updated_count} ç­†ï¼Œè·³é {skipped_count} ç­†ï¼ŒéŒ¯èª¤ {errors_count} ç­†"
            )
            return True

        except Exception as e:
            self.log_time(f"âŒ åŒ¯å…¥æ¡è³¼åˆ°è²¨é€²åº¦è³‡æ–™ç™¼ç”Ÿæœ€å¤–å±¤åš´é‡éŒ¯èª¤ï¼š{str(e)}")
            return False

    def convert_sales_ai_data(self):
        """è½‰æ› AI å°ˆç”¨éŠ·å”®è³‡æ–™ (åˆä½µç™¼è²¨ç‹€æ³åˆ†æè¡¨èˆ‡æ˜‡å³°éŠ·å”®è³‡æ–™)"""
        self.log_time("ğŸ”„ é–‹å§‹è½‰æ› AI å°ˆç”¨éŠ·å”®è³‡æ–™ (sales_ai.db)...")

        try:
            # è®€å–éŠ·å”®è³‡æ–™æª”æ¡ˆ
            df_shipment_path = os.path.join(self.data_source_dir, "ç™¼è²¨ç‹€æ³åˆ†æè¡¨.xlsx")
            df_shengfeng_path = os.path.join(self.data_source_dir, "æ˜‡å³°éŠ·å”®è³‡æ–™.xlsx")

            df_list = []

            if os.path.exists(df_shipment_path):
                df_shipment = pd.read_excel(df_shipment_path)
                df_list.append(df_shipment)
                self.log_time(f"ğŸ“Š ç™¼è²¨ç‹€æ³åˆ†æè¡¨ï¼š{len(df_shipment)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shipment_path}")

            if os.path.exists(df_shengfeng_path):
                df_shengfeng = pd.read_excel(df_shengfeng_path)
                df_list.append(df_shengfeng)
                self.log_time(f"ğŸ“Š æ˜‡å³°éŠ·å”®è³‡æ–™ï¼š{len(df_shengfeng)} ç­†")
            else:
                self.log_time(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{df_shengfeng_path}")

            if not df_list:
                self.log_time("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•éŠ·å”®è³‡æ–™æª”æ¡ˆ")
                return

            # è½‰æ›æ—¥æœŸæ™‚é–“
            # é€™è£¡éœ€è¦è™•ç†æ‰€æœ‰å¯èƒ½çš„æ—¥æœŸæ¬„ä½ï¼Œç¢ºä¿ AI èƒ½æ­£ç¢ºæŸ¥è©¢
            date_columns = ["ç™¼è²¨æ—¥æœŸ", "å–®æ“šæ—¥æœŸ", "å»ºç«‹æ—¥æœŸ", "éå¸³æ—¥æœŸ"]
            for i, df in enumerate(df_list):
                 # ä½¿ç”¨æ—¢æœ‰çš„æ—¥æœŸè½‰æ›é‚è¼¯
                if i == 0:  # ç™¼è²¨ç‹€æ³åˆ†æè¡¨
                     df_list[i] = self.convert_datetime_optimized(df, date_columns)
                else: # æ˜‡å³°éŠ·å”®è³‡æ–™ (é€šå¸¸æœ‰ç‰¹æ®Šæ—¥æœŸæ ¼å¼)
                     df_list[i] = self.convert_shengfeng_dates(df, date_columns)

            # åˆä½µè³‡æ–™ (concat æœƒè‡ªå‹•è™•ç†æ¬„ä½ä¸ä¸€è‡´ï¼Œç¼ºå°‘çš„æ¬„ä½è£œ NaN)
            df_combined = pd.concat(df_list, ignore_index=True)
            self.log_time(f"ğŸ“Š åˆä½µå¾Œç¸½ç­†æ•¸ï¼š{len(df_combined)}")

            # --- è³‡æ–™æ¸…ç†é–‹å§‹ ---
            
            # 1. æ¬„ä½åç¨±å»ç©ºç™½
            df_combined.columns = [str(col).strip() for col in df_combined.columns]

            # 2. è™•ç† 'å®¢æˆ¶' èˆ‡ 'å®¢æˆ¶åç¨±'
            # å¦‚æœæœ‰ 'å®¢æˆ¶' æ¬„ä½ï¼Œå°‡å…¶å…§å®¹å¡«å…¥ 'å®¢æˆ¶åç¨±' (å¦‚æœ 'å®¢æˆ¶åç¨±' ç‚ºç©ºæˆ–ä¸å­˜åœ¨)
            if 'å®¢æˆ¶' in df_combined.columns:
                if 'å®¢æˆ¶åç¨±' not in df_combined.columns:
                    df_combined.rename(columns={'å®¢æˆ¶': 'å®¢æˆ¶åç¨±'}, inplace=True)
                else:
                    # å…©è€…ä¸¦å­˜æ™‚ï¼Œå„ªå…ˆä¿ç•™ 'å®¢æˆ¶åç¨±'ï¼Œè‹¥ç‚ºç©ºå‰‡ç”¨ 'å®¢æˆ¶' å¡«è£œ
                    df_combined['å®¢æˆ¶åç¨±'] = df_combined['å®¢æˆ¶åç¨±'].fillna(df_combined['å®¢æˆ¶'])
                    # ç§»é™¤å¤šé¤˜çš„ 'å®¢æˆ¶' æ¬„ä½
                    df_combined.drop(columns=['å®¢æˆ¶'], inplace=True)
            
            # 3. ç§»é™¤ Unnamed æ¬„ä½
            cols_to_drop = [col for col in df_combined.columns if 'Unnamed' in col]
            if cols_to_drop:
                df_combined.drop(columns=cols_to_drop, inplace=True)
                self.log_time(f"ğŸ§¹ å·²ç§»é™¤ç„¡æ•ˆæ¬„ä½: {cols_to_drop}")

            # 4. æ•¸å€¼æ¬„ä½æ¸…ç† (ç§»é™¤é€—è™Ÿä¸¦è½‰ç‚ºæ•¸å­—)
            numeric_cols = ['äº¤æ˜“æ•¸é‡', 'å€‰åº«ç¢ºèªæ•¸é‡', 'äº¤æ˜“åƒ¹']
            for col in numeric_cols:
                if col in df_combined.columns:
                    # å…ˆè½‰ç‚ºå­—ä¸²ï¼Œç§»é™¤é€—è™Ÿï¼Œå†è½‰ç‚ºæ•¸å€¼
                    df_combined[col] = df_combined[col].astype(str).str.replace(',', '', regex=False)
                    df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce').fillna(0)
                    self.log_time(f"ğŸ”¢ å·²æ¸…ç†æ•¸å€¼æ¬„ä½: {col}")

            # --- è³‡æ–™æ¸…ç†çµæŸ ---

            # ç§»é™¤å‚™è¨»ä¸­çš„é›»è©±é€£å­—è™Ÿ (é¸ç”¨ï¼Œç‚ºäº†ä¿æŒèˆ‡ sales.db ä¸€è‡´)
            # df_combined = self.normalize_remark_columns(df_combined, ["å‚™è¨»"])

            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = sqlite3.connect(self.sales_ai_db_path)
            cursor = conn.cursor()

            # ä½¿ç”¨åˆ†æ‰¹æ’å…¥æ–¹æ³•ï¼Œå°‡æ‰€æœ‰è³‡æ–™å¯«å…¥å–®ä¸€è³‡æ–™è¡¨ 'sales_data'
            # åˆªé™¤èˆŠè¡¨
            cursor.execute("DROP TABLE IF EXISTS sales_data")
            
            # å› ç‚ºæ¬„ä½å¯èƒ½å¾ˆå¤šä¸”ä¸å›ºå®šï¼Œä½¿ç”¨ to_sql ç›´æ¥å»ºç«‹
            self.insert_data_in_chunks(df_combined, conn, "sales_data", chunk_size=1000)

            # å»ºç«‹ç´¢å¼• (é‡å°å¯èƒ½çš„å¸¸ç”¨æŸ¥è©¢æ¬„ä½)
            # å…ˆæª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
            potential_indexes = [
                ("å–®æ“šç·¨è™Ÿ", "idx_sales_ai_doc_no"),
                ("ç™¼è²¨æ—¥æœŸ", "idx_sales_ai_date"),
                ("å®¢æˆ¶åç¨±", "idx_sales_ai_customer"),
                ("ç”¢å“åç¨±", "idx_sales_ai_product"),
                ("æ¥­å‹™äººå“¡åç¨±", "idx_sales_ai_salesperson"),
                ("å®¢æˆ¶ä»£ç¢¼", "idx_sales_ai_customer_code"),
            ]
            
            valid_indexes = []
            for col, idx_name in potential_indexes:
                if col in df_combined.columns:
                    valid_indexes.append((col, idx_name))
            
            self.create_indexes_batch(cursor, "sales_data", valid_indexes)

            conn.commit()
            conn.close()

            self.log_time(f"âœ… AI éŠ·å”®è³‡æ–™è½‰æ›å®Œæˆï¼š{len(df_combined)} ç­† -> {self.sales_ai_db_path}")

        except Exception as e:
            self.log_time(f"âŒ è½‰æ› AI éŠ·å”®è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")


    def verify_data_integrity(self):
        """é©—è­‰è³‡æ–™å®Œæ•´æ€§"""
        self.log_time("?? é©—è­‰è³‡æ–™å®Œæ•´æ€§...")

        # éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è³‡æ–™åº«
        sales_databases = [
            (self.sales_db_path, "sales_main", "éŠ·å”®ä¸»æª”"),
            (self.sales_db_path, "sales_detail", "éŠ·å”®æ˜ç´°"),
            (self.repair_db_path, "repair_data", "ç¶­ä¿®è³‡æ–™"),
            (self.custody_db_path, "custody_main", "å¯„å€‰ä¸»æª”"),
            (self.custody_db_path, "custody_detail", "å¯„å€‰æ˜ç´°"),
            (self.customer_db_path, "customer_basic", "å®¢æˆ¶åŸºæœ¬è³‡æ–™"),
            (self.service_card_db_path, "service_card_data", "æœå‹™ç™»è¨˜å¡è³‡æ–™"),
            (self.crm_db_path, "crm_notes", "CRMè¨˜äº‹"),
            (self.sales_ai_db_path, "sales_data", "AIéŠ·å”®è³‡æ–™"), # Add AI sales data
        ]

        # åº«å­˜æŸ¥è©¢ç³»çµ±è³‡æ–™åº«
        inventory_inquiry_databases = [
            (self.inventory_data_db_path, "inventory_data", "æ­£èˆªåº«å­˜è³‡æ–™")
        ]

        # æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ±
        procuretrack_databases = [
            (self.procuretrack_db_path, "procure_data", "æ¡è³¼åˆ°è²¨è³‡æ–™") # Changed table name to procure_data
        ]

        self.log_time("?? éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±ï¼š")
        for db_path, table_name, description in sales_databases:
            if os.path.exists(db_path):
                try:
                    conn = sqlite3.connect(db_path)
                    cursor = conn.cursor()
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    conn.close()
                    self.log_time(f"   ? {description}ï¼š{count:,} ç­†")
                except Exception as e:
                    self.log_time(f"   ? {description} é©—è­‰å¤±æ•—ï¼š{e}")
            else:
                self.log_time(f"   ?? æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼š{db_path}")

        self.log_time("?? åº«å­˜æŸ¥è©¢ç³»çµ±ï¼š")
        for db_path, table_name, description in inventory_inquiry_databases:
            if os.path.exists(db_path):
                try:
                    conn = sqlite3.connect(db_path)
                    cursor = conn.cursor()
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    conn.close()
                    self.log_time(f"   ? {description}ï¼š{count:,} ç­†")
                except Exception as e:
                    self.log_time(f"   ? {description} é©—è­‰å¤±æ•—ï¼š{e}")
            else:
                self.log_time(f"   ?? æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼š{db_path}")

        self.log_time("?? æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ±ï¼š")
        for db_path, table_name, description in procuretrack_databases:
            if os.path.exists(db_path):
                try:
                    conn = sqlite3.connect(db_path)
                    cursor = conn.cursor()
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    conn.close()
                    self.log_time(f"   ? {description}ï¼š{count:,} ç­†")
                except Exception as e:
                    self.log_time(f"   ? {description} é©—è­‰å¤±æ•—ï¼š{e}")
            else:
                self.log_time(f"   ?? æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼š{db_path}")

    def run_bi_full_conversion(self):
        """åŸ·è¡ŒBIç³»çµ±å®Œæ•´çš„è³‡æ–™è½‰æ›æµç¨‹ï¼ˆåŒ…å«æ‰€æœ‰åˆ†æ”¯ç³»çµ±ï¼‰"""
        start_time = time.time()
        self.log_time("ğŸš€ é–‹å§‹BIç³»çµ±æ•´åˆè³‡æ–™è½‰æ›æµç¨‹...")

        try:
            # 1. è½‰æ›éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è³‡æ–™
            self.log_time("ğŸ“Š === éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ± ===")
            self.convert_customer_data()
            self.convert_sales_data()
            self.convert_repair_data()
            self.convert_custody_data()
            self.convert_crm_notes()
            self.convert_service_card_data()
            self.convert_sales_ai_data() # Add AI sales data conversion

            # 2. æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ±
            self.log_time("ğŸ›¥ === æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤ç³»çµ± ===")
            self.convert_procuretrack_data()

            # 3. è½‰æ›åº«å­˜æŸ¥è©¢ç³»çµ±è³‡æ–™
            self.log_time("ğŸ“‹ === åº«å­˜æŸ¥è©¢ç³»çµ± ===")
            self.convert_inventory_inquiry_data()

            # 4. é©—è­‰è³‡æ–™å®Œæ•´æ€§
            self.verify_data_integrity()

            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = time.time()
            total_time = end_time - start_time
            minutes = int(total_time // 60)
            seconds = int(total_time % 60)

            self.log_time(f"ğŸ‰ BIç³»çµ±è³‡æ–™è½‰æ›å®Œæˆï¼ç¸½è€—æ™‚ï¼š{minutes}åˆ†{seconds}ç§’")

        except Exception as e:
            self.log_time(f"âŒ BIç³»çµ±è³‡æ–™è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            raise

    def run_bi_daily_update(self):
        """BIç³»çµ±æ—¥å¸¸æ›´æ–°æ¨¡å¼"""
        start_time = time.time()
        self.log_time("ğŸ“… é–‹å§‹BIç³»çµ±æ—¥å¸¸æ›´æ–°æ¨¡å¼...")

        try:
            # 1. æ›´æ–°éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±
            self.log_time("ğŸ“Š === æ›´æ–°éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ± ===")
            self.convert_customer_data()
            self.convert_sales_data()
            self.convert_repair_data()
            self.convert_custody_data()
            self.convert_service_card_data()
            self.convert_sales_ai_data() # Add AI sales data conversion

            # 2. æ›´æ–°æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤è³‡æ–™
            self.log_time("ğŸ›¥ === æ›´æ–°æ¡è³¼åˆ°è²¨é€²åº¦è¿½è¹¤è³‡æ–™ ===")
            self.convert_procuretrack_data()

            # 3. æ›´æ–°åº«å­˜æŸ¥è©¢ç³»çµ±è³‡æ–™
            self.log_time("ğŸ“‹ === æ›´æ–°åº«å­˜æŸ¥è©¢ç³»çµ±è³‡æ–™ ===")
            self.convert_inventory_inquiry_data()

            # 4. é©—è­‰æ›´æ–°çµæœ
            self.verify_data_integrity()

            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = time.time()
            total_time = end_time - start_time
            minutes = int(total_time // 60)
            seconds = int(total_time % 60)

            self.log_time(f"ğŸ“… BIç³»çµ±æ—¥å¸¸æ›´æ–°å®Œæˆï¼è€—æ™‚ï¼š{minutes}åˆ†{seconds}ç§’")

        except Exception as e:
            self.log_time(f"âŒ BIç³»çµ±æ—¥å¸¸æ›´æ–°éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            raise

    def run_sales_only_conversion(self):
        """åƒ…è½‰æ›éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è³‡æ–™"""
        start_time = time.time()
        self.log_time("ğŸ“Š é–‹å§‹éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è³‡æ–™è½‰æ›...")

        try:
            # è½‰æ›éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è³‡æ–™
            self.convert_customer_data()
            self.convert_sales_data()
            self.convert_repair_data()
            self.convert_crm_notes()
            self.convert_custody_data()
            self.convert_service_card_data()
            self.convert_sales_ai_data() # Add AI sales data conversion

            # é©—è­‰çµæœ
            self.log_time("ğŸ“Š éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±ï¼š")
            sales_databases = [
                (self.sales_db_path, "sales_main", "éŠ·å”®ä¸»æª”"),
                (self.sales_db_path, "sales_detail", "éŠ·å”®æ˜ç´°"),
                (self.repair_db_path, "repair_data", "ç¶­ä¿®è³‡æ–™"),
                (self.custody_db_path, "custody_main", "å¯„å€‰ä¸»æª”"),
                (self.custody_db_path, "custody_detail", "å¯„å€‰æ˜ç´°"),
                (self.customer_db_path, "customer_basic", "å®¢æˆ¶åŸºæœ¬è³‡æ–™"),
                (self.service_card_db_path, "service_card_data", "æœå‹™ç™»è¨˜å¡è³‡æ–™"),
                (self.crm_db_path, "crm_notes", "CRMè¨˜äº‹"),
                (self.sales_ai_db_path, "sales_data", "AIéŠ·å”®è³‡æ–™"), # Add AI sales data
            ]

            for db_path, table_name, description in sales_databases:
                if os.path.exists(db_path):
                    try:
                        conn = sqlite3.connect(db_path)
                        cursor = conn.cursor()
                        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                        count = cursor.fetchone()[0]
                        conn.close()
                        self.log_time(f"   âœ… {description}ï¼š{count:,} ç­†")
                    except Exception as e:
                        self.log_time(f"   âŒ {description} é©—è­‰å¤±æ•—ï¼š{e}")
                else:
                    self.log_time(f"   âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼š{db_path}")

            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = time.time()
            total_time = end_time - start_time
            minutes = int(total_time // 60)
            seconds = int(total_time % 60)
            self.log_time(f"ğŸ“Š éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è½‰æ›å®Œæˆï¼è€—æ™‚ï¼š{minutes}åˆ†{seconds}ç§’")

        except Exception as e:
            self.log_time(f"âŒ éŠ·å”®è³‡è¨ŠæŸ¥è©¢ç³»çµ±è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            raise



def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    import sys

    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1 and sys.argv[1] == "--daily":
        # ç›´æ¥åŸ·è¡Œæ—¥å¸¸æ›´æ–°ï¼Œä¸éœ€è¦ç”¨æˆ¶è¼¸å…¥
        print("=" * 60)
        print("ğŸ”§ BIç³»çµ±æ•´åˆè³‡æ–™åŒ¯å…¥å·¥å…· - æ—¥å¸¸æ›´æ–°æ¨¡å¼")
        print("=" * 60)

        try:
            converter = BIIntegratedDataConverter()
            converter.run_bi_daily_update()
            print("\n" + "=" * 60)
            print("âœ¨ BIç³»çµ±æ—¥å¸¸æ›´æ–°å®Œæˆï¼")
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return

    # äº’å‹•æ¨¡å¼
    print("=" * 60)
    print("ğŸ”§ BIç³»çµ±æ•´åˆè³‡æ–™åŒ¯å…¥å·¥å…·")
    print("1. BIæ•´åˆä¸€æ¬¡è½‰æ›ï¼ˆå«éŠ·å”®/æ¡è³¼/åº«å­˜æŸ¥è©¢/CRMï¼‰â€”â€”å»ºè­°åˆæ¬¡ä½¿ç”¨")
    print("2. BIæ¯æ—¥æ›´æ–°ï¼ˆé©ç”¨æ¯æ—¥ä¾‹è¡Œæ›´æ–°ï¼‰")
    print("3. åƒ…è½‰æ›éŠ·å”®æŸ¥è©¢ç³»çµ±ï¼ˆå®¢æˆ¶/éŠ·å”®/ç¶­ä¿®/å¯„å€‰/æœå‹™å¡/CRMè¨˜äº‹ï¼‰")
    print("4. åƒ…è½‰æ›åº«å­˜æŸ¥è©¢ç³»çµ±ï¼ˆæ­£èˆªåº«å­˜è³‡æ–™ï¼‰")
    print("5. è½‰æ›å®¢æˆ¶è³‡æ–™")
    print("6. è½‰æ›éŠ·å”®è³‡æ–™")
    print("7. è½‰æ›ç¶­ä¿®è³‡æ–™")
    print("8. è½‰æ›å¯„å€‰è³‡æ–™")
    print("9. è½‰æ›æœå‹™å¡è³‡æ–™")
    print("10. åŒ¯å…¥ CRM è¨˜äº‹è³‡æ–™ (CRMè¨˜äº‹.db)")
    print("11. åŒ¯å…¥ æ¡è³¼åˆ°è²¨é€²åº¦è³‡æ–™ (procure.db)")
    print("12. æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§")
    print("13. åŒ¯å…¥ AI å°ˆç”¨éŠ·å”®è³‡æ–™ (sales_ai.db) [å«ç™¼è²¨ç‹€æ³èˆ‡æ˜‡å³°éŠ·å”®]")
    print("Q. é›¢é–‹")
    print("=" * 60)
    print("ğŸ“Œ ä½¿ç”¨å»ºè­°")
    print("   é¸é …1ï¼šå»ºè­°é¦–æ¬¡ä½¿ç”¨æˆ–è³‡æ–™ç•°å‹•å¾Œå®Œæ•´é‡å»º")
    print("   é¸é …2ï¼šæ¯æ—¥ä¾‹è¡Œæ›´æ–°ï¼ˆé¿å…é‡æ–°åŒ¯å…¥æ‰€æœ‰è³‡æ–™ï¼‰")
    print("   é¸é …3-4ï¼šé‡å°ç‰¹å®šç³»çµ±å¿«é€Ÿæ›´æ–°")
    print("   é¸é …5-11ï¼šé‡å°å–®ä¸€ä¸»æª”é‡æ–°åŒ¯å…¥")
    print("   é¸é …12ï¼šæª¢æŸ¥å„ç³»çµ±è³‡æ–™ç­†æ•¸æ˜¯å¦æ­£å¸¸")
    print("   é¸é …13ï¼šç‚º AI æ‡‰ç”¨æº–å‚™çš„éŠ·å”®è³‡æ–™ï¼Œå¯ç¨ç«‹æ›´æ–°")
    print("=" * 60)
    print("ğŸ“ è³‡æ–™ä¾†æºç›®éŒ„ï¼šD:\\WEB\\BI\\è³‡æ–™ä¾†æº")
    print("=" * 60)

    try:
        choice = input("è«‹é¸æ“‡æ“ä½œæ¨¡å¼ (1-12): ").strip()

        converter = BIIntegratedDataConverter()

        if choice == "1":
            converter.run_bi_full_conversion()
        elif choice == "2":
            converter.run_bi_daily_update()
        elif choice == "3":
            converter.run_sales_only_conversion()
        elif choice == "4":
            converter.convert_inventory_inquiry_data()
        elif choice == "5":
            converter.convert_customer_data()
        elif choice == "6":
            converter.convert_sales_data()
        elif choice == "7":
            converter.convert_repair_data()
        elif choice == "8":
            converter.convert_custody_data()
        elif choice == "9":
            converter.convert_service_card_data()
        elif choice == "10":
            converter.convert_crm_notes()
        elif choice == "11":
            converter.convert_procuretrack_data()
        elif choice == "12":
            converter.verify_data_integrity()
        elif choice == "13":
            converter.convert_sales_ai_data()
            return

        print("\n" + "=" * 60)
        print("âœ¨ æ“ä½œå®Œæˆï¼")

    except KeyboardInterrupt:
        print("\nâŒ æ“ä½œè¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    finally:
        try:
            input("\næŒ‰ Enter éµçµæŸ...")
        except EOFError:
            # è™•ç†æ‰¹æ¬¡æª”åŸ·è¡Œæ™‚çš„ EOF éŒ¯èª¤
            pass



if __name__ == "__main__":
    main()
